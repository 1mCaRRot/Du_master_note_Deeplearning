{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "10c0525c",
   "metadata": {},
   "source": [
    "# 一 导包及常用API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a872ea6",
   "metadata": {},
   "source": [
    "# 1 导入Scikit-learn库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0a128cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd1e6d98",
   "metadata": {},
   "source": [
    "# 2 Bunch格式说明"
   ]
  },
  {
   "cell_type": "raw",
   "id": "37ec5c61",
   "metadata": {},
   "source": [
    "Bunch格式包括下述内容：\n",
    "\n",
    "1.数据（data）：通常是一个二维ndarray数组或矩阵，包含特征数据。\n",
    "2.目标（target）：包含与数据对应的标签或目标值，通常为二维ndarray数组。\n",
    "3.特征名称（feature_names）：可选的，描述数据中每个特征的名称。\n",
    "4.目标名称（target_names）：可选的，描述目标变量的名称。\n",
    "5.描述（DESCR）：可选的，关于数据集和其特征的描述性文本。\n",
    "6.其他元数据：可能包括其他信息，如样本数量、数据集来源等。\n",
    "\n",
    "Bunch 类似于字典，可以使用点操作符访问其属性，例如 bunch.data 或 bunch.target。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c04a3064",
   "metadata": {},
   "source": [
    "# 3 导入小规模数据集，数据包含在datasets里"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5cbe14df",
   "metadata": {},
   "source": [
    "语法： from sklearn.datasets import load_*\n",
    "\n",
    "说明：上面的星号对应的是需要导入的内容，返回的数据类型为：datasets.base.Bunch（字典格式）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bd1ce90",
   "metadata": {},
   "source": [
    "## 3.1 导入鸢尾花数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "89813d7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "鸢尾花数据集的内容为：\n",
      "{'data': array([[5.1, 3.5, 1.4, 0.2],\n",
      "       [4.9, 3. , 1.4, 0.2],\n",
      "       [4.7, 3.2, 1.3, 0.2],\n",
      "       [4.6, 3.1, 1.5, 0.2],\n",
      "       [5. , 3.6, 1.4, 0.2],\n",
      "       [5.4, 3.9, 1.7, 0.4],\n",
      "       [4.6, 3.4, 1.4, 0.3],\n",
      "       [5. , 3.4, 1.5, 0.2],\n",
      "       [4.4, 2.9, 1.4, 0.2],\n",
      "       [4.9, 3.1, 1.5, 0.1],\n",
      "       [5.4, 3.7, 1.5, 0.2],\n",
      "       [4.8, 3.4, 1.6, 0.2],\n",
      "       [4.8, 3. , 1.4, 0.1],\n",
      "       [4.3, 3. , 1.1, 0.1],\n",
      "       [5.8, 4. , 1.2, 0.2],\n",
      "       [5.7, 4.4, 1.5, 0.4],\n",
      "       [5.4, 3.9, 1.3, 0.4],\n",
      "       [5.1, 3.5, 1.4, 0.3],\n",
      "       [5.7, 3.8, 1.7, 0.3],\n",
      "       [5.1, 3.8, 1.5, 0.3],\n",
      "       [5.4, 3.4, 1.7, 0.2],\n",
      "       [5.1, 3.7, 1.5, 0.4],\n",
      "       [4.6, 3.6, 1. , 0.2],\n",
      "       [5.1, 3.3, 1.7, 0.5],\n",
      "       [4.8, 3.4, 1.9, 0.2],\n",
      "       [5. , 3. , 1.6, 0.2],\n",
      "       [5. , 3.4, 1.6, 0.4],\n",
      "       [5.2, 3.5, 1.5, 0.2],\n",
      "       [5.2, 3.4, 1.4, 0.2],\n",
      "       [4.7, 3.2, 1.6, 0.2],\n",
      "       [4.8, 3.1, 1.6, 0.2],\n",
      "       [5.4, 3.4, 1.5, 0.4],\n",
      "       [5.2, 4.1, 1.5, 0.1],\n",
      "       [5.5, 4.2, 1.4, 0.2],\n",
      "       [4.9, 3.1, 1.5, 0.2],\n",
      "       [5. , 3.2, 1.2, 0.2],\n",
      "       [5.5, 3.5, 1.3, 0.2],\n",
      "       [4.9, 3.6, 1.4, 0.1],\n",
      "       [4.4, 3. , 1.3, 0.2],\n",
      "       [5.1, 3.4, 1.5, 0.2],\n",
      "       [5. , 3.5, 1.3, 0.3],\n",
      "       [4.5, 2.3, 1.3, 0.3],\n",
      "       [4.4, 3.2, 1.3, 0.2],\n",
      "       [5. , 3.5, 1.6, 0.6],\n",
      "       [5.1, 3.8, 1.9, 0.4],\n",
      "       [4.8, 3. , 1.4, 0.3],\n",
      "       [5.1, 3.8, 1.6, 0.2],\n",
      "       [4.6, 3.2, 1.4, 0.2],\n",
      "       [5.3, 3.7, 1.5, 0.2],\n",
      "       [5. , 3.3, 1.4, 0.2],\n",
      "       [7. , 3.2, 4.7, 1.4],\n",
      "       [6.4, 3.2, 4.5, 1.5],\n",
      "       [6.9, 3.1, 4.9, 1.5],\n",
      "       [5.5, 2.3, 4. , 1.3],\n",
      "       [6.5, 2.8, 4.6, 1.5],\n",
      "       [5.7, 2.8, 4.5, 1.3],\n",
      "       [6.3, 3.3, 4.7, 1.6],\n",
      "       [4.9, 2.4, 3.3, 1. ],\n",
      "       [6.6, 2.9, 4.6, 1.3],\n",
      "       [5.2, 2.7, 3.9, 1.4],\n",
      "       [5. , 2. , 3.5, 1. ],\n",
      "       [5.9, 3. , 4.2, 1.5],\n",
      "       [6. , 2.2, 4. , 1. ],\n",
      "       [6.1, 2.9, 4.7, 1.4],\n",
      "       [5.6, 2.9, 3.6, 1.3],\n",
      "       [6.7, 3.1, 4.4, 1.4],\n",
      "       [5.6, 3. , 4.5, 1.5],\n",
      "       [5.8, 2.7, 4.1, 1. ],\n",
      "       [6.2, 2.2, 4.5, 1.5],\n",
      "       [5.6, 2.5, 3.9, 1.1],\n",
      "       [5.9, 3.2, 4.8, 1.8],\n",
      "       [6.1, 2.8, 4. , 1.3],\n",
      "       [6.3, 2.5, 4.9, 1.5],\n",
      "       [6.1, 2.8, 4.7, 1.2],\n",
      "       [6.4, 2.9, 4.3, 1.3],\n",
      "       [6.6, 3. , 4.4, 1.4],\n",
      "       [6.8, 2.8, 4.8, 1.4],\n",
      "       [6.7, 3. , 5. , 1.7],\n",
      "       [6. , 2.9, 4.5, 1.5],\n",
      "       [5.7, 2.6, 3.5, 1. ],\n",
      "       [5.5, 2.4, 3.8, 1.1],\n",
      "       [5.5, 2.4, 3.7, 1. ],\n",
      "       [5.8, 2.7, 3.9, 1.2],\n",
      "       [6. , 2.7, 5.1, 1.6],\n",
      "       [5.4, 3. , 4.5, 1.5],\n",
      "       [6. , 3.4, 4.5, 1.6],\n",
      "       [6.7, 3.1, 4.7, 1.5],\n",
      "       [6.3, 2.3, 4.4, 1.3],\n",
      "       [5.6, 3. , 4.1, 1.3],\n",
      "       [5.5, 2.5, 4. , 1.3],\n",
      "       [5.5, 2.6, 4.4, 1.2],\n",
      "       [6.1, 3. , 4.6, 1.4],\n",
      "       [5.8, 2.6, 4. , 1.2],\n",
      "       [5. , 2.3, 3.3, 1. ],\n",
      "       [5.6, 2.7, 4.2, 1.3],\n",
      "       [5.7, 3. , 4.2, 1.2],\n",
      "       [5.7, 2.9, 4.2, 1.3],\n",
      "       [6.2, 2.9, 4.3, 1.3],\n",
      "       [5.1, 2.5, 3. , 1.1],\n",
      "       [5.7, 2.8, 4.1, 1.3],\n",
      "       [6.3, 3.3, 6. , 2.5],\n",
      "       [5.8, 2.7, 5.1, 1.9],\n",
      "       [7.1, 3. , 5.9, 2.1],\n",
      "       [6.3, 2.9, 5.6, 1.8],\n",
      "       [6.5, 3. , 5.8, 2.2],\n",
      "       [7.6, 3. , 6.6, 2.1],\n",
      "       [4.9, 2.5, 4.5, 1.7],\n",
      "       [7.3, 2.9, 6.3, 1.8],\n",
      "       [6.7, 2.5, 5.8, 1.8],\n",
      "       [7.2, 3.6, 6.1, 2.5],\n",
      "       [6.5, 3.2, 5.1, 2. ],\n",
      "       [6.4, 2.7, 5.3, 1.9],\n",
      "       [6.8, 3. , 5.5, 2.1],\n",
      "       [5.7, 2.5, 5. , 2. ],\n",
      "       [5.8, 2.8, 5.1, 2.4],\n",
      "       [6.4, 3.2, 5.3, 2.3],\n",
      "       [6.5, 3. , 5.5, 1.8],\n",
      "       [7.7, 3.8, 6.7, 2.2],\n",
      "       [7.7, 2.6, 6.9, 2.3],\n",
      "       [6. , 2.2, 5. , 1.5],\n",
      "       [6.9, 3.2, 5.7, 2.3],\n",
      "       [5.6, 2.8, 4.9, 2. ],\n",
      "       [7.7, 2.8, 6.7, 2. ],\n",
      "       [6.3, 2.7, 4.9, 1.8],\n",
      "       [6.7, 3.3, 5.7, 2.1],\n",
      "       [7.2, 3.2, 6. , 1.8],\n",
      "       [6.2, 2.8, 4.8, 1.8],\n",
      "       [6.1, 3. , 4.9, 1.8],\n",
      "       [6.4, 2.8, 5.6, 2.1],\n",
      "       [7.2, 3. , 5.8, 1.6],\n",
      "       [7.4, 2.8, 6.1, 1.9],\n",
      "       [7.9, 3.8, 6.4, 2. ],\n",
      "       [6.4, 2.8, 5.6, 2.2],\n",
      "       [6.3, 2.8, 5.1, 1.5],\n",
      "       [6.1, 2.6, 5.6, 1.4],\n",
      "       [7.7, 3. , 6.1, 2.3],\n",
      "       [6.3, 3.4, 5.6, 2.4],\n",
      "       [6.4, 3.1, 5.5, 1.8],\n",
      "       [6. , 3. , 4.8, 1.8],\n",
      "       [6.9, 3.1, 5.4, 2.1],\n",
      "       [6.7, 3.1, 5.6, 2.4],\n",
      "       [6.9, 3.1, 5.1, 2.3],\n",
      "       [5.8, 2.7, 5.1, 1.9],\n",
      "       [6.8, 3.2, 5.9, 2.3],\n",
      "       [6.7, 3.3, 5.7, 2.5],\n",
      "       [6.7, 3. , 5.2, 2.3],\n",
      "       [6.3, 2.5, 5. , 1.9],\n",
      "       [6.5, 3. , 5.2, 2. ],\n",
      "       [6.2, 3.4, 5.4, 2.3],\n",
      "       [5.9, 3. , 5.1, 1.8]]), 'target': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]), 'frame': None, 'target_names': array(['setosa', 'versicolor', 'virginica'], dtype='<U10'), 'DESCR': '.. _iris_dataset:\\n\\nIris plants dataset\\n--------------------\\n\\n**Data Set Characteristics:**\\n\\n    :Number of Instances: 150 (50 in each of three classes)\\n    :Number of Attributes: 4 numeric, predictive attributes and the class\\n    :Attribute Information:\\n        - sepal length in cm\\n        - sepal width in cm\\n        - petal length in cm\\n        - petal width in cm\\n        - class:\\n                - Iris-Setosa\\n                - Iris-Versicolour\\n                - Iris-Virginica\\n                \\n    :Summary Statistics:\\n\\n    ============== ==== ==== ======= ===== ====================\\n                    Min  Max   Mean    SD   Class Correlation\\n    ============== ==== ==== ======= ===== ====================\\n    sepal length:   4.3  7.9   5.84   0.83    0.7826\\n    sepal width:    2.0  4.4   3.05   0.43   -0.4194\\n    petal length:   1.0  6.9   3.76   1.76    0.9490  (high!)\\n    petal width:    0.1  2.5   1.20   0.76    0.9565  (high!)\\n    ============== ==== ==== ======= ===== ====================\\n\\n    :Missing Attribute Values: None\\n    :Class Distribution: 33.3% for each of 3 classes.\\n    :Creator: R.A. Fisher\\n    :Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)\\n    :Date: July, 1988\\n\\nThe famous Iris database, first used by Sir R.A. Fisher. The dataset is taken\\nfrom Fisher\\'s paper. Note that it\\'s the same as in R, but not as in the UCI\\nMachine Learning Repository, which has two wrong data points.\\n\\nThis is perhaps the best known database to be found in the\\npattern recognition literature.  Fisher\\'s paper is a classic in the field and\\nis referenced frequently to this day.  (See Duda & Hart, for example.)  The\\ndata set contains 3 classes of 50 instances each, where each class refers to a\\ntype of iris plant.  One class is linearly separable from the other 2; the\\nlatter are NOT linearly separable from each other.\\n\\n.. topic:: References\\n\\n   - Fisher, R.A. \"The use of multiple measurements in taxonomic problems\"\\n     Annual Eugenics, 7, Part II, 179-188 (1936); also in \"Contributions to\\n     Mathematical Statistics\" (John Wiley, NY, 1950).\\n   - Duda, R.O., & Hart, P.E. (1973) Pattern Classification and Scene Analysis.\\n     (Q327.D83) John Wiley & Sons.  ISBN 0-471-22361-1.  See page 218.\\n   - Dasarathy, B.V. (1980) \"Nosing Around the Neighborhood: A New System\\n     Structure and Classification Rule for Recognition in Partially Exposed\\n     Environments\".  IEEE Transactions on Pattern Analysis and Machine\\n     Intelligence, Vol. PAMI-2, No. 1, 67-71.\\n   - Gates, G.W. (1972) \"The Reduced Nearest Neighbor Rule\".  IEEE Transactions\\n     on Information Theory, May 1972, 431-433.\\n   - See also: 1988 MLC Proceedings, 54-64.  Cheeseman et al\"s AUTOCLASS II\\n     conceptual clustering system finds 3 classes in the data.\\n   - Many, many more ...', 'feature_names': ['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)'], 'filename': 'iris.csv', 'data_module': 'sklearn.datasets.data'}\n"
     ]
    }
   ],
   "source": [
    "# 导入鸢尾花数据集\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "# 数据集实例化\n",
    "iris = load_iris()\n",
    "\n",
    "# 输出鸢尾花数据集\n",
    "print(f\"鸢尾花数据集的内容为：\\n{iris}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaed9928",
   "metadata": {},
   "source": [
    "## 3.2 导入红酒数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d114ba1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "红酒数据集的内容为：\n",
      "{'data': array([[1.423e+01, 1.710e+00, 2.430e+00, ..., 1.040e+00, 3.920e+00,\n",
      "        1.065e+03],\n",
      "       [1.320e+01, 1.780e+00, 2.140e+00, ..., 1.050e+00, 3.400e+00,\n",
      "        1.050e+03],\n",
      "       [1.316e+01, 2.360e+00, 2.670e+00, ..., 1.030e+00, 3.170e+00,\n",
      "        1.185e+03],\n",
      "       ...,\n",
      "       [1.327e+01, 4.280e+00, 2.260e+00, ..., 5.900e-01, 1.560e+00,\n",
      "        8.350e+02],\n",
      "       [1.317e+01, 2.590e+00, 2.370e+00, ..., 6.000e-01, 1.620e+00,\n",
      "        8.400e+02],\n",
      "       [1.413e+01, 4.100e+00, 2.740e+00, ..., 6.100e-01, 1.600e+00,\n",
      "        5.600e+02]]), 'target': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2,\n",
      "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "       2, 2]), 'frame': None, 'target_names': array(['class_0', 'class_1', 'class_2'], dtype='<U7'), 'DESCR': '.. _wine_dataset:\\n\\nWine recognition dataset\\n------------------------\\n\\n**Data Set Characteristics:**\\n\\n    :Number of Instances: 178\\n    :Number of Attributes: 13 numeric, predictive attributes and the class\\n    :Attribute Information:\\n \\t\\t- Alcohol\\n \\t\\t- Malic acid\\n \\t\\t- Ash\\n\\t\\t- Alcalinity of ash  \\n \\t\\t- Magnesium\\n\\t\\t- Total phenols\\n \\t\\t- Flavanoids\\n \\t\\t- Nonflavanoid phenols\\n \\t\\t- Proanthocyanins\\n\\t\\t- Color intensity\\n \\t\\t- Hue\\n \\t\\t- OD280/OD315 of diluted wines\\n \\t\\t- Proline\\n\\n    - class:\\n            - class_0\\n            - class_1\\n            - class_2\\n\\t\\t\\n    :Summary Statistics:\\n    \\n    ============================= ==== ===== ======= =====\\n                                   Min   Max   Mean     SD\\n    ============================= ==== ===== ======= =====\\n    Alcohol:                      11.0  14.8    13.0   0.8\\n    Malic Acid:                   0.74  5.80    2.34  1.12\\n    Ash:                          1.36  3.23    2.36  0.27\\n    Alcalinity of Ash:            10.6  30.0    19.5   3.3\\n    Magnesium:                    70.0 162.0    99.7  14.3\\n    Total Phenols:                0.98  3.88    2.29  0.63\\n    Flavanoids:                   0.34  5.08    2.03  1.00\\n    Nonflavanoid Phenols:         0.13  0.66    0.36  0.12\\n    Proanthocyanins:              0.41  3.58    1.59  0.57\\n    Colour Intensity:              1.3  13.0     5.1   2.3\\n    Hue:                          0.48  1.71    0.96  0.23\\n    OD280/OD315 of diluted wines: 1.27  4.00    2.61  0.71\\n    Proline:                       278  1680     746   315\\n    ============================= ==== ===== ======= =====\\n\\n    :Missing Attribute Values: None\\n    :Class Distribution: class_0 (59), class_1 (71), class_2 (48)\\n    :Creator: R.A. Fisher\\n    :Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)\\n    :Date: July, 1988\\n\\nThis is a copy of UCI ML Wine recognition datasets.\\nhttps://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data\\n\\nThe data is the results of a chemical analysis of wines grown in the same\\nregion in Italy by three different cultivators. There are thirteen different\\nmeasurements taken for different constituents found in the three types of\\nwine.\\n\\nOriginal Owners: \\n\\nForina, M. et al, PARVUS - \\nAn Extendible Package for Data Exploration, Classification and Correlation. \\nInstitute of Pharmaceutical and Food Analysis and Technologies,\\nVia Brigata Salerno, 16147 Genoa, Italy.\\n\\nCitation:\\n\\nLichman, M. (2013). UCI Machine Learning Repository\\n[https://archive.ics.uci.edu/ml]. Irvine, CA: University of California,\\nSchool of Information and Computer Science. \\n\\n.. topic:: References\\n\\n  (1) S. Aeberhard, D. Coomans and O. de Vel, \\n  Comparison of Classifiers in High Dimensional Settings, \\n  Tech. Rep. no. 92-02, (1992), Dept. of Computer Science and Dept. of  \\n  Mathematics and Statistics, James Cook University of North Queensland. \\n  (Also submitted to Technometrics). \\n\\n  The data was used with many others for comparing various \\n  classifiers. The classes are separable, though only RDA \\n  has achieved 100% correct classification. \\n  (RDA : 100%, QDA 99.4%, LDA 98.9%, 1NN 96.1% (z-transformed data)) \\n  (All results using the leave-one-out technique) \\n\\n  (2) S. Aeberhard, D. Coomans and O. de Vel, \\n  \"THE CLASSIFICATION PERFORMANCE OF RDA\" \\n  Tech. Rep. no. 92-01, (1992), Dept. of Computer Science and Dept. of \\n  Mathematics and Statistics, James Cook University of North Queensland. \\n  (Also submitted to Journal of Chemometrics).\\n', 'feature_names': ['alcohol', 'malic_acid', 'ash', 'alcalinity_of_ash', 'magnesium', 'total_phenols', 'flavanoids', 'nonflavanoid_phenols', 'proanthocyanins', 'color_intensity', 'hue', 'od280/od315_of_diluted_wines', 'proline']}\n",
      "\n",
      "红酒数据集的特征名字内容为：\n",
      "['alcohol', 'malic_acid', 'ash', 'alcalinity_of_ash', 'magnesium', 'total_phenols', 'flavanoids', 'nonflavanoid_phenols', 'proanthocyanins', 'color_intensity', 'hue', 'od280/od315_of_diluted_wines', 'proline']\n"
     ]
    }
   ],
   "source": [
    "# 导入红酒数据集\n",
    "from sklearn.datasets import load_wine\n",
    "\n",
    "# 数据集实例化\n",
    "wine = load_wine()\n",
    "\n",
    "# 输出红酒数据集\n",
    "print(f\"红酒数据集的内容为：\\n{wine}\\n\")\n",
    "\n",
    "# 由于wine是一个Bunch类型的数据，所以可以使用类似字典键值对的访问方式\n",
    "print(f\"红酒数据集的特征名字内容为：\\n{wine.feature_names}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "706afca8",
   "metadata": {},
   "source": [
    "# 4 导入大规模数据集，需要从网上下载"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a5cbdd5f",
   "metadata": {},
   "source": [
    "语法： from sklearn.datasets import fetch_*\n",
    "\n",
    "说明：上面的星号对应的是需要导入的内容，返回的数据类型为：datasets.base.Bunch（字典格式）\n",
    "\n",
    "由于大规模数据集的使用方式与小规模数据集相似，故而不再进行演示"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7d9f19f",
   "metadata": {},
   "source": [
    "# 二 特征工程"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05a8614a",
   "metadata": {},
   "source": [
    "# 1 数据集的划分"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "123bb74e",
   "metadata": {},
   "source": [
    "## API：sklearn.model_selection"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6b548f6a",
   "metadata": {},
   "source": [
    "机器学习的数据通常会划分为两类：\n",
    "1.训练数据：用于训练，构建模型\n",
    "2.测试数据：在模拟检验时使用，用于评估模型是否有效"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e61b77b",
   "metadata": {},
   "source": [
    "## sklearn.model_selection.train_test_split方法"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a700d992",
   "metadata": {},
   "source": [
    "语法：x_train, x_test = sklearn.model_selection.train_test_split(*arrays, test_size=0.25, random_state=37)\n",
    "\n",
    "参数说明：*arrays：可以传入多个数组，特别需要注意的是，前面需要用两个变量接收，即训练集和测试集（先训练再测试）。\n",
    "       test_size：默认值为0.25。如果是float，表示测试集占总数据集的比例（0 到 1 之间）。如果是int，表示测试集的样本数量。\n",
    "       random_state: 默认值为None。随机数种子，用于保证样本的一致，传入int类型即可。\n",
    "\n",
    "功能：将输入特征数据（如 X）和目标变量（如 y）划分为训练集和测试集，以便后续模型训练和评估。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0b2eb5c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原数据特征值数组的形状为：(150, 4)\n",
      "训练集的形状为：(112, 4)\n",
      "测试集的形状为：(38, 4)\n"
     ]
    }
   ],
   "source": [
    "# 导入鸢尾花数据集\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "# 导入数据集划分API\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 数据集实例化\n",
    "iris = load_iris()\n",
    "\n",
    "# 对鸢尾花数据集进行划分\n",
    "x_train, x_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.25, random_state=37)\n",
    "\n",
    "# 检验划分结果，这里正确的结果应该是测试集比例占25%\n",
    "print(f\"原数据特征值数组的形状为：{iris.data.shape}\")\n",
    "print(f\"训练集的形状为：{x_train.shape}\")\n",
    "print(f\"测试集的形状为：{x_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52fdd0d1",
   "metadata": {},
   "source": [
    "# 2 特征提取"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f95bf762",
   "metadata": {},
   "source": [
    "## API：sklearn.feature_extraction"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6e1d4d82",
   "metadata": {},
   "source": [
    "将任意数据（如文本或图像）转换为可以用于机器学习的数字特征"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61c73ade",
   "metadata": {},
   "source": [
    "## 2.1 字典特征提取: sklearn.feature_extraction.DictVectorizer类"
   ]
  },
  {
   "cell_type": "raw",
   "id": "40c5131f",
   "metadata": {},
   "source": [
    "英语：sparse (a.)稀疏的\n",
    "\n",
    "语法：sklearn.feature_extraction.DictVectorizer(sparse=True, dtype=float64)\n",
    "\n",
    "导包语法：from sklearn.feature_extraction import DictVectorizer\n",
    "\n",
    "参数说明： sparse: 默认为 True，表示返回稀疏矩阵。如果设置为 False，则返回一个密集矩阵。\n",
    "        dtype:  指定输出特征矩阵的数据类型，默认是 np.float64。\n",
    "\n",
    "功能：将字典格式的数据转换为特征矩阵，适用于处理稀疏特征。它能够将包含特征的字典转化为适合于机器学习模型的数值形式。\n",
    "\n",
    "注：sklearn.feature_extraction.DictVectorizer 是一个类，而不是单独的方法。这个类提供了一些方法用于将字典格式的数据转换为特征矩阵。\n",
    "   因为DictVectorizer是一个类，故而在使用时需要先实例化！！"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f14a4f83",
   "metadata": {},
   "source": [
    "稀疏矩阵：适合用于大规模数据集，尤其是当大多数元素为零时。使用稀疏矩阵可以显著节省内存和存储空间。\n",
    "密集矩阵：适合用于相对较小且大多数元素为非零的矩阵。由于其存储方式简单，计算速度较快。"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7144cade",
   "metadata": {},
   "source": [
    "方法\n",
    "fit(X): 计算特征矩阵的特征集。\n",
    "transform(X): 将字典数据转换为特征矩阵。\n",
    "fit_transform(X): 同时执行 fit 和 transform。\n",
    "get_feature_names_out(): 获取特征名称。\n",
    "\n",
    "参数说明：\n",
    "X：应为一个列表，其中每个元素是一个字典（dict）。字典的键是特征名，值是特征值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "69889161",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  1.  0. 29.]\n",
      " [ 1.  0.  0. 36.]\n",
      " [ 0.  0.  1. 32.]]\n",
      "['city=上海' 'city=北京' 'city=广州' '温度']\n"
     ]
    }
   ],
   "source": [
    "# 导入类对象\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "\n",
    "# 准备示例数据\n",
    "data = [{\"city\": \"北京\", \"温度\": 29}, {\"city\": \"上海\", \"温度\": 36}, {\"city\": \"广州\", \"温度\": 32}]\n",
    "\n",
    "# 创建 DictVectorizer 实例\n",
    "vectorizer = DictVectorizer(sparse=False)\n",
    "\n",
    "# 使用fit_transform方法，将转换列表内的字典数据为特征矩阵\n",
    "X = vectorizer.fit_transform(data)\n",
    "\n",
    "# 输出特征矩阵和特征名称\n",
    "print(X)\n",
    "print(vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0e8ed6f",
   "metadata": {},
   "source": [
    "## 2.2 文本特征提取：sklearn.feature_extraction.text.CountVectorizer类（计数）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e722af5e",
   "metadata": {},
   "source": [
    "### 2.2.1 英文文本特征抽取"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3055f6f3",
   "metadata": {},
   "source": [
    "语法：sklearn.feature_extraction.text.CountVectorizer(stop_words=[])\n",
    "\n",
    "导包语法：from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "参数说明：stop_words ：停用对某些词的统计，将需要忽略的词填入列表中即可。\n",
    "\n",
    "功能：统计每一个样本出现特征词的次数。\n",
    "\n",
    "\n",
    "实例化后的常用类方法\n",
    "1. fit(raw_documents) \n",
    "参数说明:  raw_documents: 文档列表或可迭代对象。\n",
    "功能：学习词汇表，并返回 self。\n",
    "\n",
    "2. transform(raw_documents)\n",
    "参数说明:  raw_documents: 待转换的文档列表。\n",
    "功能：将新文档转换为特征向量（稀疏矩阵）。\n",
    "\n",
    "3. fit_transform(raw_documents)\n",
    "参数说明: raw_documents: 文档列表。\n",
    "功能：同时进行 fit 和 transform，返回特征矩阵（sparse矩阵）。\n",
    "\n",
    "4. get_feature_names_out()\n",
    "功能：返回学习到的特征名称（词汇表）。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c50cb232",
   "metadata": {},
   "source": [
    "特别注意：单个字母 和 标点符号 不会被计数。\n",
    "       在使用中文时候，需要进行空格分开，不然会识别一整句话作为一个元素进行。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "743f06c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 0)\t1\n",
      "  (0, 5)\t1\n",
      "  (0, 4)\t1\n",
      "  (0, 1)\t1\n",
      "  (0, 3)\t1\n",
      "  (1, 5)\t1\n",
      "  (1, 1)\t2\n",
      "  (1, 3)\t1\n",
      "  (1, 2)\t1\n",
      "['life' 'like' 'math' 'python' 'short' 'too']\n"
     ]
    }
   ],
   "source": [
    "# 导入类对象\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# 准备示例数据\n",
    "data = [\"life is too short, i like python\", \"i like python, and i like math, too!\"]\n",
    "\n",
    "# 创建 CountVectorizer 实例，同时停用 and 和 is\n",
    "vectorizer = CountVectorizer(stop_words=[\"and\", \"is\"])\n",
    "\n",
    "# 使用fit_transform方法，将转换列表内的字符串数据为特征矩阵\n",
    "X = vectorizer.fit_transform(data)\n",
    "\n",
    "# 输出特征矩阵和特征名称\n",
    "print(X)\n",
    "print(vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d76e7d1e",
   "metadata": {},
   "source": [
    "## 补充： toarray方法"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2f37f947",
   "metadata": {},
   "source": [
    ".toarray() 方法是 scipy.sparse 矩阵的一个功能，通常用于将稀疏矩阵转换为密集的 NumPy 数组。\n",
    "\n",
    "语法：sparse_matrix.toarray()\n",
    "\n",
    "功能：运行上述代码后，你将得到特征名称以及对应的密集矩阵，显示每个单词在各个文档中的出现次数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "694151ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 1 1 0 1 1 1]\n",
      " [1 0 0 2 1 1 0 1]]\n",
      "['and' 'is' 'life' 'like' 'math' 'python' 'short' 'too']\n"
     ]
    }
   ],
   "source": [
    "# 导入类对象\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# 准备示例数据\n",
    "data = [\"life is too short, i like python\", \"i like python, and i like math, too!\"]\n",
    "\n",
    "# 创建 CountVectorizer 实例\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# 使用fit_transform方法，将转换列表内的字符串数据为特征矩阵\n",
    "X = vectorizer.fit_transform(data)\n",
    "\n",
    "# 输出特征矩阵和特征名称\n",
    "print(X.toarray())\n",
    "print(vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e2c3fd5",
   "metadata": {},
   "source": [
    "## 2.2.2 中文文本特征抽取"
   ]
  },
  {
   "cell_type": "raw",
   "id": "972ae29d",
   "metadata": {},
   "source": [
    "使用 jieba 库中的 jieba.cut()方法 处理中文文本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c4126b4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Dumping model to file cache C:\\Users\\Lenovo\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 1.028 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0]\n",
      " [0 0 0]\n",
      " [1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 0]\n",
      " [0 1 0]\n",
      " [0 0 0]\n",
      " [0 0 1]]\n",
      "['北京' '天安门' '太阳升']\n"
     ]
    }
   ],
   "source": [
    "# 先导包\n",
    "import jieba\n",
    "\n",
    "# 导入类对象\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# 准备示例数据\n",
    "text = \"我爱北京天安门，天安门上太阳升\"\n",
    "\n",
    "# 创建 CountVectorizer 实例\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# 使用fit_transform方法，将转换列表内的字符串数据为特征矩阵，注意列表中的字符串先进性了断句处理\n",
    "X = vectorizer.fit_transform(jieba.cut(text))\n",
    "\n",
    "# 输出特征矩阵和特征名称\n",
    "print(X.toarray())\n",
    "print(vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc0345ac",
   "metadata": {},
   "source": [
    "## 2.3 文本特征提取：sklearn.feature_extraction.text.TfidfVectorizer类（检索、分类）"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7c32dfcb",
   "metadata": {},
   "source": [
    "导包语法：from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "参数说明：stop_words ：停用对某些词的统计，将需要忽略的词填入列表中即可。\n",
    "\n",
    "功能：将文本转换为数值特征，便于后续的机器学习模型训练和分析。\n",
    "\n",
    "实例化后的常用类方法\n",
    "1. fit(raw_documents) \n",
    "参数说明:  raw_documents: 文档列表或可迭代对象。\n",
    "功能：学习词汇表，并返回 self。\n",
    "\n",
    "2. transform(raw_documents)\n",
    "参数说明:  raw_documents: 待转换的文档列表。\n",
    "功能：将新文档转换为特征向量（稀疏矩阵）。\n",
    "\n",
    "3. fit_transform(raw_documents)\n",
    "参数说明: raw_documents: 文档列表。\n",
    "功能：同时进行 fit 和 transform，返回特征矩阵（sparse矩阵）。\n",
    "\n",
    "4. get_feature_names_out()\n",
    "功能：返回学习到的特征名称（词汇表）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8e8ddcd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 3)\t0.3793034928087496\n",
      "  (0, 1)\t0.3793034928087496\n",
      "  (0, 4)\t0.5330978245262535\n",
      "  (0, 5)\t0.3793034928087496\n",
      "  (0, 0)\t0.5330978245262535\n",
      "  (1, 2)\t0.4976748316029239\n",
      "  (1, 3)\t0.3540997415957358\n",
      "  (1, 1)\t0.7081994831914716\n",
      "  (1, 5)\t0.3540997415957358\n",
      "['life' 'like' 'math' 'python' 'short' 'too']\n"
     ]
    }
   ],
   "source": [
    "# 导入类对象\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# 准备示例数据\n",
    "data = [\"life is too short, i like python\", \"i like python, and i like math, too!\"]\n",
    "\n",
    "# 创建 CountVectorizer 实例，同时停用 and 和 is\n",
    "vectorizer = TfidfVectorizer(stop_words=[\"and\", \"is\"])\n",
    "\n",
    "# 使用fit_transform方法，将转换列表内的字符串数据为特征矩阵\n",
    "X = vectorizer.fit_transform(data)\n",
    "\n",
    "# 输出特征矩阵和特征名称\n",
    "print(X)\n",
    "print(vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c52c620b",
   "metadata": {},
   "source": [
    "# 3 特征预处理"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a4e2da07",
   "metadata": {},
   "source": [
    "导包语法：from sklearn.preprocessing import *\n",
    "\n",
    "说明： sklearn.preprocessing是预处理的模块\n",
    "     这里的 * 就是各种类、方法对象，在下面会进行介绍"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6632bee8",
   "metadata": {},
   "source": [
    "## 3.1 无量纲化（适用于不同数组间 数值差异过大）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "152f1803",
   "metadata": {},
   "source": [
    "### 3.1.1 数据归一化：MinMaxScaler类"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35d22dfd",
   "metadata": {},
   "source": [
    "归一化：通过对原始数据进行变换把数据映射到[0, 1]（默认）之间"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c8a353de",
   "metadata": {},
   "source": [
    "导包语法：from sklearn.preprocessing import MinMaxScaler\n",
    "参数说明：feature_range: 可以自定义缩放的范围，默认是 [0, 1]。例如，可以设置为 (-1, 1)。\n",
    "\n",
    "方法： fit_transform\n",
    "语法： 实例化对象.fit_transform(data)\n",
    "功能：该方法首先计算特征的最小值和最大值，然后根据这些值将数据缩放到指定范围。\n",
    "\n",
    "计算公式：{x' = x-X(min)} / {X(max) - X(min)}   这里x指的是每一个元素，X是同类别所有的元素\n",
    "注：可以看出，当x是最大值时，得到的数为1，当x是最小值是，得到的数是0。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3b7f3ec0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原数据内容为：\n",
      "     milage     Liters  Consumtime\n",
      "0     40920   8.326976    0.953952\n",
      "1     14488   7.153469    1.673904\n",
      "2     26052   1.441871    0.805124\n",
      "3     75136  13.147394    0.428964\n",
      "4     38344   1.669788    0.134296\n",
      "..      ...        ...         ...\n",
      "995   11145   3.410627    0.631838\n",
      "996   68846   9.974715    0.669787\n",
      "997   26575  10.650102    0.866627\n",
      "998   48111   9.134528    0.728045\n",
      "999   43757   7.882601    1.332446\n",
      "\n",
      "[1000 rows x 3 columns]\n",
      "\n",
      "处理后的数据内容为：\n",
      "[[0.44832535 0.39805139 0.56233353]\n",
      " [0.15873259 0.34195467 0.98724416]\n",
      " [0.28542943 0.06892523 0.47449629]\n",
      " ...\n",
      " [0.29115949 0.50910294 0.51079493]\n",
      " [0.52711097 0.43665451 0.4290048 ]\n",
      " [0.47940793 0.3768091  0.78571804]]\n"
     ]
    }
   ],
   "source": [
    "# 先导包\n",
    "import pandas as pd\n",
    "\n",
    "# 导入数据预处理包\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# 实例化\n",
    "mms = MinMaxScaler()\n",
    "\n",
    "# 读取数据\n",
    "f = pd.read_csv(\"练习数据.csv\")\n",
    "\n",
    "# 选择待处理数据\n",
    "data = f.iloc[: , :3]\n",
    "\n",
    "# 使用fit_transform方法将数据归一化\n",
    "data_new = mms.fit_transform(data)\n",
    "\n",
    "# 对比\n",
    "print(f\"原数据内容为：\\n{data}\\n\\n处理后的数据内容为：\\n{data_new}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9156e1f4",
   "metadata": {},
   "source": [
    "### 3.1.2 数据标准化：StandardScaler类"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24907c08",
   "metadata": {},
   "source": [
    "标准差：standard deviation\n",
    "\n",
    "标准化：将数据转换为均值(mean)为 0、标准差(std)为 1 的标准正态分布。"
   ]
  },
  {
   "cell_type": "raw",
   "id": "315971fe",
   "metadata": {},
   "source": [
    "导包语法：from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "方法： fit_transform\n",
    "语法： 实例化对象.fit_transform(data)\n",
    "功能：该方法首先计算特征的均值和标准差，然后根据这些值将数据标准化。\n",
    "\n",
    "计算公式：X' = {x - mean} / std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "da4aa1b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原数据内容为：\n",
      "     milage     Liters  Consumtime\n",
      "0     40920   8.326976    0.953952\n",
      "1     14488   7.153469    1.673904\n",
      "2     26052   1.441871    0.805124\n",
      "3     75136  13.147394    0.428964\n",
      "4     38344   1.669788    0.134296\n",
      "..      ...        ...         ...\n",
      "995   11145   3.410627    0.631838\n",
      "996   68846   9.974715    0.669787\n",
      "997   26575  10.650102    0.866627\n",
      "998   48111   9.134528    0.728045\n",
      "999   43757   7.882601    1.332446\n",
      "\n",
      "[1000 rows x 3 columns]\n",
      "\n",
      "处理后的数据内容为：\n",
      "[[ 0.33193158  0.41660188  0.24523407]\n",
      " [-0.87247784  0.13992897  1.69385734]\n",
      " [-0.34554872 -1.20667094 -0.05422437]\n",
      " ...\n",
      " [-0.32171752  0.96431572  0.06952649]\n",
      " [ 0.65959911  0.60699509 -0.20931587]\n",
      " [ 0.46120328  0.31183342  1.00680598]]\n",
      "\n",
      "标准化后数据的均值为：-2.4276876805136757e-17\n",
      "标准差为：1.0\n"
     ]
    }
   ],
   "source": [
    "# 先导包\n",
    "import pandas as pd\n",
    "\n",
    "# 导入数据预处理包\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# 实例化\n",
    "sds = StandardScaler()\n",
    "\n",
    "# 读取数据\n",
    "f = pd.read_csv(\"练习数据.csv\")\n",
    "\n",
    "# 选择待处理数据\n",
    "data = f.iloc[: , :3]\n",
    "\n",
    "# 使用fit_transform方法将数据归一化\n",
    "data_new = sds.fit_transform(data)\n",
    "\n",
    "# 对比\n",
    "print(f\"原数据内容为：\\n{data}\\n\\n处理后的数据内容为：\\n{data_new}\")\n",
    "\n",
    "# 检查标准化后数据的均值和标准差\n",
    "import numpy as np\n",
    "print(f\"\\n标准化后数据的均值为：{np.mean(data_new)}\\n标准差为：{np.std(data_new)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddb28ee3",
   "metadata": {},
   "source": [
    "# 4 特征降维"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5d5db361",
   "metadata": {},
   "source": [
    "说明：降维指的是在某些限定条件下，降低随机变量（特征）个数，得到一组“不相关”主变量的过程。\n",
    "\n",
    "注： 通常情况下，数据集以二维数组的形式表示，其中行代表样本，列代表特征。\n",
    "    特征降维并不局限于二维数组，而是可以应用于任意维度的数据。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "863f4ba4",
   "metadata": {},
   "source": [
    "## 4.1 特征选择"
   ]
  },
  {
   "cell_type": "raw",
   "id": "572c4094",
   "metadata": {},
   "source": [
    "from sklearn.feature_selection import *\n",
    "\n",
    "说明： sklearn.feature_selection是特征选择的模块\n",
    "     这里的 * 就是各种类、方法对象，在下面会进行介绍"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "881c2759",
   "metadata": {},
   "source": [
    "### 4.1.1 Filter过滤式：主要探究特征本身特点、特征与特征和目标值之间关联"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b938aef",
   "metadata": {},
   "source": [
    "#### 4.1.1.1 方差选择法：低方差特征过滤 VarianceThreshold方法"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3276d8a4",
   "metadata": {},
   "source": [
    "英语： threshold (n.)阈值\n",
    "\n",
    "导包语法：from sklearn.feature_selection import VarianceThreshold\n",
    "参数：threshold ：设置特征方差的阈值。低于该阈值的特征将被移除。可以使用浮点数值来指定具体的方差阈值，默认值为0.0\n",
    "\n",
    "方法： fit_transform\n",
    "语法： 实例化对象.fit_transform(data)\n",
    "功能： 进行特征选择，即去除 低方差特征。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7d8a1a79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原数据内容为：\n",
      "(2318, 9)\n",
      "\n",
      "处理后的数据内容为：\n",
      "(2318, 6)\n"
     ]
    }
   ],
   "source": [
    "# 先导包\n",
    "import pandas as pd\n",
    "\n",
    "# 导入特征选择模块中的 低方差特征删除工具\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "# 实例化类对象，并设置阈值为37\n",
    "vt = VarianceThreshold(threshold=37)\n",
    "\n",
    "# 读取数据\n",
    "f = pd.read_csv(\"factor_returns.csv\")\n",
    "data = f.iloc[: , 1: 10]\n",
    "\n",
    "# 使用fit_transform方法对数据进行 低方差过滤\n",
    "data_new = vt.fit_transform(data)\n",
    "\n",
    "# 对比\n",
    "print(f\"原数据内容为：\\n{data.shape}\\n\\n处理后的数据内容为：\\n{data_new.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dd2e3dd",
   "metadata": {},
   "source": [
    "#### 4.1.1.2 相关系数：特征与特征之间的相关程度"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b9cd74da",
   "metadata": {},
   "source": [
    "皮尔逊相关系数（Pearson Correlation Coefficient）：反映变量之间相关关系密切程度的统计指标，取值范围为：[-1, 1]\n",
    "\n",
    "如果r > 0，说明两变量正相关，即一方增长，另一个也增长。\n",
    "如果r < 0，说明两变量负相关，即一方增长，另一个减少。\n",
    "如果r = 0，说明两变量无相关性。\n",
    "\n",
    "也可以进行划分，如: |r|<0.4为低度相关，0.4<=|r|<0.7为显著性相关，0.7<=|r|<1为高度相关"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7db39261",
   "metadata": {},
   "source": [
    "导包语法：from scipy.stats import pearsonr\n",
    "\n",
    "语法：correlation_coefficient, p_value = pearsonr(x, y)\n",
    "\n",
    "参数说明：需要使用两个变量进行接收，第一个是相关系数，第二个是p值\n",
    "       x和y是准备计算的两个内容，可以是数组类型或者类数组类型（列表等）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0aefaa18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x和y的皮尔逊相关系数为：1.0，p值为：0.0\n"
     ]
    }
   ],
   "source": [
    "# 先导包\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "# 示例数据\n",
    "x = [1, 2, 3, 4, 5]\n",
    "y = [2, 4, 6, 8, 10]\n",
    "\n",
    "# 计算相关系数和 p 值\n",
    "correlation_coefficient, p_value = pearsonr(x, y)\n",
    "\n",
    "# 输出结果\n",
    "print(f\"x和y的皮尔逊相关系数为：{correlation_coefficient}，p值为：{p_value}\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "85925323",
   "metadata": {},
   "source": [
    "特征与特征之间的相关性很高，有如下三种处理方式：\n",
    "1.选取其中一个\n",
    "2.加权求和\n",
    "3.主成分分析"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0db2e9a1",
   "metadata": {},
   "source": [
    "### 4.1.2 Embedded嵌入式：算法自动选择特征、特征与目标值之间的关联"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c04aee4c",
   "metadata": {},
   "source": [
    "#### 4.1.2.1 决策树"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "782bfb4d",
   "metadata": {},
   "source": [
    "#### 4.1.2.2 正则化"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c79b800",
   "metadata": {},
   "source": [
    "#### 4.1.2.3 深度学习"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b28329ce",
   "metadata": {},
   "source": [
    "## 4.2 主成分分析（PCA：Principal Component Analysis）"
   ]
  },
  {
   "cell_type": "raw",
   "id": "822dd96f",
   "metadata": {},
   "source": [
    "定义：高维数据转化为低维数据的过程，在此过程中可能会舍弃原有数据、创造新的变量\n",
    "\n",
    "作用：进行数据维数压缩，尽可能降低原数据的维数（复杂度），损失少量信息。\n",
    "\n",
    "应用：回归分析、聚类分析等"
   ]
  },
  {
   "cell_type": "raw",
   "id": "da6ffac3",
   "metadata": {},
   "source": [
    "from sklearn.decomposition import *\n",
    "\n",
    "说明： sklearn.decomposition是降维的模块\n",
    "     这里的 * 就是各种类、方法对象，在下面会进行介绍"
   ]
  },
  {
   "cell_type": "raw",
   "id": "90e67280",
   "metadata": {},
   "source": [
    "导包语法：from sklearn.decomposition import PCA\n",
    "参数说明：n_components   小数：表示保留百分之多少的信息（0-1之间的数）\n",
    "                  整数：减少到多少特征\n",
    "\n",
    "方法：fit_transform(X) \n",
    "语法：实例化对象.fit_transform(X)\n",
    "参数说明：X：Numpyarray格式的数据\n",
    "功能：将数据分解为较低维空间"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6298e5bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.82797019],\n",
       "       [ 1.77758033],\n",
       "       [-0.99219749],\n",
       "       [-0.27421042],\n",
       "       [-1.67580142],\n",
       "       [-0.9129491 ],\n",
       "       [ 0.09910944],\n",
       "       [ 1.14457216],\n",
       "       [ 0.43804614],\n",
       "       [ 1.22382056]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 导入主成分分析类\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# 类对象实例化，这里希望保留原本的90%\n",
    "pca = PCA(n_components=0.9)\n",
    "\n",
    "# 准备数据\n",
    "X = np.array([[2.5, 2.4], [0.5, 0.7], [2.2, 2.9], [1.9, 2.2], [3.1, 3.0], [2.3, 2.7], [2, 1.6], [1, 1.1], [1.5, 1.6], [1.1, 0.9]])\n",
    "\n",
    "# 进行主成分分析\n",
    "X_new = pca.fit_transform(X)\n",
    "X_new"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e49dd8a6",
   "metadata": {},
   "source": [
    "# 三 分类算法"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0824aa4c",
   "metadata": {},
   "source": [
    "# 1 K-近邻算法（KNN算法：K-Nearest Neighbors）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "286d863a",
   "metadata": {},
   "source": [
    "根据输入数据点的 K 个最近邻居的类别或数值来进行预测。\n",
    "\n",
    "K值取得过小，容易受到异常值的影响；K值取得过大，若样本不均衡，也容易出现偏差。故而需要合理调整k值的取值。"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6d4ef379",
   "metadata": {},
   "source": [
    "导包语法：from sklearn.neighbors import KNeighborsClassifier(n_neighbors=5, algorithm=\"auto\")\n",
    "参数说明：n_neighbors  可选，int类型，默认值为5，k_neighbors查询默认使用的邻居数\n",
    "       algorithm   可选，默认值为\"auto\"。用于计算最近邻居的算法，包括下述内容：\n",
    "               \"ball_tree\"：使用 BallTree 数据结构。\n",
    "                \"kd_tree\"：使用 KDTree 数据结构。\n",
    "                \"brute\"：使用暴力搜索。\n",
    "                \"auto\"：自动选择最适合的算法。\n",
    "\n",
    "方法：\n",
    "1.fit(self, X, y)\n",
    "参数： X：特征数据，形状为 (n_samples, n_features)。\n",
    "     y：目标标签，形状为 (n_samples,)。\n",
    "用途：训练 KNN 分类器。\n",
    "\n",
    "\n",
    "2.predict(self, X)\n",
    "参数： X：待预测的特征数据，形状为 (n_samples, n_features)。\n",
    "返回： 预测的标签数组。\n",
    "用途： 对新样本进行预测。\n",
    "\n",
    "\n",
    "3.score(self, X, y)\n",
    "用途：计算模型的准确率。\n",
    "参数： X：特征数据，形状为 (n_samples, n_features)。\n",
    "     y：真实标签，形状为 (n_samples,)。\n",
    "返回：准确率（浮点数）。\n",
    "\n",
    "4.kneighbors(self, X=None, n_neighbors=None, return_distance=True)\n",
    "用途：查找给定点的 K 个最近邻居。\n",
    "参数： X：待查询的点，形状为 (n_samples, n_features)。\n",
    "     n_neighbors：要返回的邻居数量（默认为 self.n_neighbors）。\n",
    "     return_distance：是否返回距离（默认为 True）。\n",
    "返回：邻居的距离和索引。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "309711ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "使用方法一直接进行比对：\n",
      "[ True  True  True  True  True  True  True  True  True False  True False\n",
      "  True  True  True  True  True  True  True  True  True  True  True False\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True]\n",
      "\n",
      "使用方法二计算准确率，得出准确率为：0.9210526315789473\n"
     ]
    }
   ],
   "source": [
    "# 1. 先导包\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# 2. 获取鸢尾花数据\n",
    "iris = load_iris()\n",
    "\n",
    "# 3. 划分数据集\n",
    "x_train, x_test, y_train, y_test = train_test_split(iris.data, iris.target, random_state=37)\n",
    "\n",
    "# 4. 特征工程：标准化\n",
    "transfer = StandardScaler()\n",
    "x_train = transfer.fit_transform(x_train)  # 训练集使用 fit_transform\n",
    "x_test = transfer.transform(x_test)        # 测试集使用 transform\n",
    "\n",
    "# 5. 估计器实例化，并对训练集进行拟合\n",
    "estimator = KNeighborsClassifier(n_neighbors=3, algorithm=\"auto\")\n",
    "estimator.fit(x_train, y_train)\n",
    "\n",
    "# 6. 模型评估\n",
    "# 方法一：直接比对真实值和预测值\n",
    "y_predict = estimator.predict(x_test)\n",
    "print(f\"使用方法一直接进行比对：\\n{y_test == y_predict}\\n\")\n",
    "# 方法二：计算准确率\n",
    "score = estimator.score(x_test, y_test)\n",
    "print(f\"使用方法二计算准确率，得出准确率为：{score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4384268b",
   "metadata": {},
   "source": [
    "# 2 模型选择与调优：sklearn.model_selection.GridSearchCV类"
   ]
  },
  {
   "cell_type": "raw",
   "id": "92a726d6",
   "metadata": {},
   "source": [
    "导包语法： from sklearn.model_selection import GridSearchCV\n",
    "参数说明： estimator：要调优的模型实例化对象。（可理解为对原对象进行“升级”）\n",
    "        param_grid：字典或列表，定义要搜索的超参数及其值的组合。\n",
    "        cv：交叉验证的折数，默认是 5，常用值为10。\n",
    "\n",
    "新方法：显示最佳参数: estimator.best_params_\n",
    "      显示最佳结果：estimator.best_score_ \n",
    "      显示最佳估计器：estimator.best_estimator_\n",
    "      显示交叉验证结果:estimator.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "91a6b3c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "使用方法一直接进行比对：\n",
      "[ True  True  True  True  True  True  True  True  True False  True False\n",
      "  True  True  True  True  True  True  True  True  True  True  True False\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True]\n",
      "\n",
      "使用方法二计算准确率，得出准确率为：0.9210526315789473\n",
      "\n",
      "最佳参数为： {'n_neighbors': 3}\n",
      "最佳结果为： 0.9545454545454547\n",
      "最佳估计器为： KNeighborsClassifier(n_neighbors=3)\n",
      "交叉验证结果:{'mean_fit_time': array([0.00079575, 0.00075214, 0.00075209, 0.00081449, 0.00084438,\n",
      "       0.00110664]), 'std_fit_time': array([0.00055698, 0.00051421, 0.0004039 , 0.00057545, 0.00044901,\n",
      "       0.00059437]), 'mean_score_time': array([0.0019006 , 0.00185361, 0.00210435, 0.00194094, 0.00180502,\n",
      "       0.0016464 ]), 'std_score_time': array([0.00057013, 0.00045066, 0.0005887 , 0.00056984, 0.00071882,\n",
      "       0.00054068]), 'param_n_neighbors': masked_array(data=[1, 3, 5, 7, 9, 11],\n",
      "             mask=[False, False, False, False, False, False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'params': [{'n_neighbors': 1}, {'n_neighbors': 3}, {'n_neighbors': 5}, {'n_neighbors': 7}, {'n_neighbors': 9}, {'n_neighbors': 11}], 'split0_test_score': array([1.        , 1.        , 1.        , 1.        , 0.91666667,\n",
      "       0.91666667]), 'split1_test_score': array([1., 1., 1., 1., 1., 1.]), 'split2_test_score': array([1.        , 0.90909091, 0.90909091, 0.90909091, 0.90909091,\n",
      "       0.90909091]), 'split3_test_score': array([1., 1., 1., 1., 1., 1.]), 'split4_test_score': array([0.81818182, 0.90909091, 0.90909091, 0.90909091, 0.90909091,\n",
      "       0.90909091]), 'split5_test_score': array([0.90909091, 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        ]), 'split6_test_score': array([0.81818182, 0.90909091, 0.81818182, 0.72727273, 0.72727273,\n",
      "       0.72727273]), 'split7_test_score': array([0.81818182, 0.81818182, 0.90909091, 0.90909091, 0.90909091,\n",
      "       0.90909091]), 'split8_test_score': array([1., 1., 1., 1., 1., 1.]), 'split9_test_score': array([1., 1., 1., 1., 1., 1.]), 'mean_test_score': array([0.93636364, 0.95454545, 0.95454545, 0.94545455, 0.93712121,\n",
      "       0.93712121]), 'std_test_score': array([0.08181818, 0.06098367, 0.06098367, 0.08331956, 0.08159692,\n",
      "       0.08159692]), 'rank_test_score': array([6, 1, 1, 3, 4, 4])}\n"
     ]
    }
   ],
   "source": [
    "# 1. 先导包\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# 2. 获取鸢尾花数据\n",
    "iris = load_iris()\n",
    "\n",
    "# 3. 划分数据集\n",
    "x_train, x_test, y_train, y_test = train_test_split(iris.data, iris.target, random_state=37)\n",
    "\n",
    "# 4. 特征工程：标准化\n",
    "transfer = StandardScaler()\n",
    "x_train = transfer.fit_transform(x_train)  # 训练集使用 fit_transform\n",
    "x_test = transfer.transform(x_test)        # 测试集使用 transform\n",
    "\n",
    "# 5. 估计器实例化，加入网格搜索与交叉验证，并对训练集进行拟合\n",
    "estimator = KNeighborsClassifier(algorithm=\"auto\")\n",
    "\n",
    "param_dict = {\"n_neighbors\": [1, 3, 5, 7, 9, 11]}\n",
    "estimator = GridSearchCV(estimator, param_grid=param_dict, cv=10) # 这里将estimator修改了，添加了网格搜索与交叉验证\n",
    "\n",
    "estimator.fit(x_train, y_train)\n",
    "\n",
    "# 6. 模型评估\n",
    "# 方法一：直接比对真实值和预测值\n",
    "y_predict = estimator.predict(x_test)\n",
    "print(f\"使用方法一直接进行比对：\\n{y_test == y_predict}\\n\")\n",
    "# 方法二：计算准确率\n",
    "score = estimator.score(x_test, y_test)\n",
    "print(f\"使用方法二计算准确率，得出准确率为：{score}\\n\")\n",
    "\n",
    "# 7. 结果分析\n",
    "print(\"最佳参数为：\", estimator.best_params_)\n",
    "print(\"最佳结果为：\", estimator.best_score_)\n",
    "print(\"最佳估计器为：\", estimator.best_estimator_)\n",
    "print(f\"交叉验证结果:{estimator.cv_results_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b37876fa",
   "metadata": {},
   "source": [
    "# 3 朴素贝叶斯算法（Naive Bayes）"
   ]
  },
  {
   "cell_type": "raw",
   "id": "05580634",
   "metadata": {},
   "source": [
    "导包语法： from sklearn.naive_bayes import *\n",
    "        这里的 * 代表需要导入的类对象，下面会有说明"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0ef18f3",
   "metadata": {},
   "source": [
    "假设事件相互独立，则称为朴素贝叶斯。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6142a38",
   "metadata": {},
   "source": [
    "## 3.1 多项式朴素贝叶斯"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b1b762ce",
   "metadata": {},
   "source": [
    "导包语法：from sklearn.naive_bayes import MultinomialNB\n",
    "功能：专门用于多项式分布的分类任务，适合离散特征。\n",
    "适用场景：常用于文本分类任务，如文档分类和垃圾邮件检测。\n",
    "特点：计算单词在每个类别中的出现频率，适合数据稀疏的情况。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2460bbda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "使用方法一直接进行比对：\n",
      "[ True  True  True ...  True  True  True]\n",
      "\n",
      "使用方法二计算准确率，得出准确率为：0.847623089983022\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 先导包\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# 准备数据\n",
    "news = fetch_20newsgroups(subset=\"all\")  # all代表训练集和测试集都有。此参数的默认为train，意味着只有训练集。\n",
    "\n",
    "# 划分数据集\n",
    "x_train, x_test, y_train, y_test = train_test_split(news.data, news.target, random_state=37)\n",
    "\n",
    "# 文本特征抽取\n",
    "transfer = TfidfVectorizer()\n",
    "x_train = transfer.fit_transform(x_train)\n",
    "x_test = transfer.transform(x_test)\n",
    "\n",
    "# 朴素贝叶斯算法\n",
    "estimator = MultinomialNB()\n",
    "estimator.fit(x_train, y_train)\n",
    "\n",
    "# 模型评估\n",
    "# 方法一：直接比对真实值和预测值\n",
    "y_predict = estimator.predict(x_test)\n",
    "print(f\"使用方法一直接进行比对：\\n{y_test == y_predict}\\n\")\n",
    "# 方法二：计算准确率\n",
    "score = estimator.score(x_test, y_test)\n",
    "print(f\"使用方法二计算准确率，得出准确率为：{score}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52f19458",
   "metadata": {},
   "source": [
    "# 4 决策树（Decision Tree）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b294e8f",
   "metadata": {},
   "source": [
    "高效地进行决策，按特征的先后顺序进行（越重要的越靠前）"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f436ad0b",
   "metadata": {},
   "source": [
    "导包语法：from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "参数说明：criterion=’entropy’   数据差异大用基尼参数，精确数据用信息熵\n",
    "       random_state=None      随机种子，可用于对比"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2154fa4f",
   "metadata": {},
   "source": [
    "## 4.1 创建决策树"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6c9c89d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "使用方法一直接进行比对：\n",
      "[ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True False\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True]\n",
      "\n",
      "使用方法二计算准确率，得出准确率为：0.9736842105263158\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 先导包\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# 准备数据\n",
    "iris = load_iris()\n",
    "\n",
    "# 划分数据集\n",
    "x_train, x_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.25, random_state=37)\n",
    "\n",
    "# 决策树预估器\n",
    "estimator = DecisionTreeClassifier()\n",
    "estimator.fit(x_train, y_train)\n",
    "\n",
    "# 模型评估\n",
    "# 方法一：直接比对真实值和预测值\n",
    "y_predict = estimator.predict(x_test)\n",
    "print(f\"使用方法一直接进行比对：\\n{y_test == y_predict}\\n\")\n",
    "# 方法二：计算准确率\n",
    "score = estimator.score(x_test, y_test)\n",
    "print(f\"使用方法二计算准确率，得出准确率为：{score}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8daa7190",
   "metadata": {},
   "source": [
    "## 4.2 使用决策树处理泰坦尼克号案例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "52187ceb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "使用方法一直接进行比对：\n",
      "645     True\n",
      "308     True\n",
      "558     True\n",
      "983     True\n",
      "647     True\n",
      "        ... \n",
      "498     True\n",
      "422     True\n",
      "1160    True\n",
      "967     True\n",
      "103     True\n",
      "Name: survived, Length: 329, dtype: bool\n",
      "\n",
      "使用方法二计算准确率，得出准确率为：0.8024316109422492\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\AppData\\Local\\Temp\\ipykernel_57156\\1144736630.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  x.loc[:, \"age\"] = x[\"age\"].fillna(x[\"age\"].mean())\n"
     ]
    }
   ],
   "source": [
    "# 先导包\n",
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "\n",
    "# 获取数据\n",
    "data = pd.read_csv(\"titanic.csv\")\n",
    "\n",
    "# 筛选特征值和目标值\n",
    "x = data[[\"pclass\", \"age\", \"sex\"]]\n",
    "y = data[\"survived\"]\n",
    "\n",
    "# 缺失值处理（因为age里面有缺失值，故而需要继续数据处理），并转换为字典格式\n",
    "x.loc[:, \"age\"] = x[\"age\"].fillna(x[\"age\"].mean())\n",
    "x = x.to_dict(orient=\"records\")\n",
    "\n",
    "# 划分数据集\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, random_state=37)\n",
    "\n",
    "# 字典特征抽取\n",
    "transfer = DictVectorizer()\n",
    "x_train = transfer.fit_transform(x_train)\n",
    "x_test = transfer.transform(x_test)\n",
    "\n",
    "# 决策树预估器\n",
    "estimator = DecisionTreeClassifier(criterion=\"entropy\")\n",
    "estimator.fit(x_train, y_train)\n",
    "\n",
    "# 模型评估\n",
    "# 方法一：直接比对真实值和预测值\n",
    "y_predict = estimator.predict(x_test)\n",
    "print(f\"使用方法一直接进行比对：\\n{y_test == y_predict}\\n\")\n",
    "# 方法二：计算准确率\n",
    "score = estimator.score(x_test, y_test)\n",
    "print(f\"使用方法二计算准确率，得出准确率为：{score}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c2b4c6b",
   "metadata": {},
   "source": [
    "# 5 随机森林"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8c08a0f",
   "metadata": {},
   "source": [
    "定义：随机森林是一种用于分类和回归任务的集成学习方法。它通过构建多个决策树，并将它们的输出合并，以提高准确性并控制过拟合。"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7273be47",
   "metadata": {},
   "source": [
    "导包语法： from sklearn.ensemble import RandomForestClassifier\n",
    "参数说明： n_estimators         要构建的树的数量（默认是 100）。\n",
    "        max_features         每棵树在节点分裂时考虑的最大特征数量。\n",
    "        max_depth           树的最大深度，可以用于控制过拟合。\n",
    "        min_samples_split      内部节点再划分所需的最小样本数。\n",
    "        min_samples_leaf       叶子节点所需的最小样本数。\n",
    "        criterion=’entropy’   数据差异大用基尼参数，精确数据用信息熵\n",
    "        random_state=None      随机种子，可用于对比"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d797857",
   "metadata": {},
   "source": [
    "## 5.1 使用随机森林对4.2的泰坦尼克号进行预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "70fc088c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\AppData\\Local\\Temp\\ipykernel_57156\\4248802369.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  x.loc[:, \"age\"] = x[\"age\"].fillna(x[\"age\"].mean())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "使用方法一直接进行比对：\n",
      "645     True\n",
      "308     True\n",
      "558     True\n",
      "983     True\n",
      "647     True\n",
      "        ... \n",
      "498     True\n",
      "422     True\n",
      "1160    True\n",
      "967     True\n",
      "103     True\n",
      "Name: survived, Length: 329, dtype: bool\n",
      "\n",
      "使用方法二计算准确率，得出准确率为：0.7993920972644377\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 先导包\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "\n",
    "# 获取数据\n",
    "data = pd.read_csv(\"titanic.csv\")\n",
    "\n",
    "# 筛选特征值和目标值\n",
    "x = data[[\"pclass\", \"age\", \"sex\"]]\n",
    "y = data[\"survived\"]\n",
    "\n",
    "# 缺失值处理（因为age里面有缺失值，故而需要继续数据处理），并转换为字典格式\n",
    "x.loc[:, \"age\"] = x[\"age\"].fillna(x[\"age\"].mean())\n",
    "x = x.to_dict(orient=\"records\")\n",
    "\n",
    "# 划分数据集\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, random_state=37)\n",
    "\n",
    "# 字典特征抽取\n",
    "transfer = DictVectorizer()\n",
    "x_train = transfer.fit_transform(x_train)\n",
    "x_test = transfer.transform(x_test)\n",
    "\n",
    "# 随机森林预估器\n",
    "estimator = RandomForestClassifier(criterion=\"entropy\")\n",
    "estimator.fit(x_train, y_train)\n",
    "\n",
    "# 模型评估\n",
    "# 方法一：直接比对真实值和预测值\n",
    "y_predict = estimator.predict(x_test)\n",
    "print(f\"使用方法一直接进行比对：\\n{y_test == y_predict}\\n\")\n",
    "# 方法二：计算准确率\n",
    "score = estimator.score(x_test, y_test)\n",
    "print(f\"使用方法二计算准确率，得出准确率为：{score}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f251689",
   "metadata": {},
   "source": [
    "## 5.2 使用调优后的随机森林对4.2的泰坦尼克号进行预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9303e59c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\AppData\\Local\\Temp\\ipykernel_57156\\1179159403.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  x.loc[:, \"age\"] = x[\"age\"].fillna(x[\"age\"].mean())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "使用方法一直接进行比对：\n",
      "645      True\n",
      "308      True\n",
      "558      True\n",
      "983      True\n",
      "647     False\n",
      "        ...  \n",
      "498      True\n",
      "422      True\n",
      "1160     True\n",
      "967      True\n",
      "103      True\n",
      "Name: survived, Length: 329, dtype: bool\n",
      "\n",
      "使用方法二计算准确率，得出准确率为：0.8389057750759878\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 先导包\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# 获取数据\n",
    "data = pd.read_csv(\"titanic.csv\")\n",
    "\n",
    "# 筛选特征值和目标值\n",
    "x = data[[\"pclass\", \"age\", \"sex\"]]\n",
    "y = data[\"survived\"]\n",
    "\n",
    "# 缺失值处理（因为age里面有缺失值，故而需要继续数据处理），并转换为字典格式\n",
    "x.loc[:, \"age\"] = x[\"age\"].fillna(x[\"age\"].mean())\n",
    "x = x.to_dict(orient=\"records\")\n",
    "\n",
    "# 划分数据集\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, random_state=37)\n",
    "\n",
    "# 字典特征抽取\n",
    "transfer = DictVectorizer()\n",
    "x_train = transfer.fit_transform(x_train)\n",
    "x_test = transfer.transform(x_test)\n",
    "\n",
    "# 随机森林预估器\n",
    "estimator = RandomForestClassifier(criterion=\"entropy\")\n",
    "\n",
    "# 加入网格搜索与交叉验证\n",
    "# 参数准备\n",
    "param_dict = {\"n_estimators\" : [1, 3, 5, 7, 9, 11], \"max_depth\" : [5, 8, 15, 25, 30]}\n",
    "estimator = GridSearchCV(estimator, param_grid=param_dict, cv=3)\n",
    "estimator.fit(x_train, y_train)\n",
    "\n",
    "# 模型评估\n",
    "# 方法一：直接比对真实值和预测值\n",
    "y_predict = estimator.predict(x_test)\n",
    "print(f\"使用方法一直接进行比对：\\n{y_test == y_predict}\\n\")\n",
    "# 方法二：计算准确率\n",
    "score = estimator.score(x_test, y_test)\n",
    "print(f\"使用方法二计算准确率，得出准确率为：{score}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be210e9d",
   "metadata": {},
   "source": [
    "# 四 回归与聚类（无监督，即无目标）算法"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8c6f7ae",
   "metadata": {},
   "source": [
    "# 1 线性回归（优化算法）"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b8f08781",
   "metadata": {},
   "source": [
    "导包语法： from sklearn.linear_model import *\n",
    "\n",
    "说明： sklearn.linear_model是线性回归的模块\n",
    "     这里的 * 就是各种类、方法对象，在下面会进行介绍"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0f2aec5",
   "metadata": {},
   "source": [
    "说明：下面的误差大，原因是因为红酒数据集中没有线性关系，因为波士顿房价已经被移除了"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb1f70c3",
   "metadata": {},
   "source": [
    "## 1.1 正规方程：sklearn.linear_model.LinearRegression（一次到位）"
   ]
  },
  {
   "cell_type": "raw",
   "id": "bdb6920d",
   "metadata": {},
   "source": [
    "导包语法：from sklearn.linear_model import LinearRegression\n",
    "参数：   fit_intercept   默认为 True，是否计算模型的截距\n",
    "       normalize      默认为 False，当 fit_intercept 设置为 False 时，此参数被忽略。\n",
    "       copy_X        默认为 True。如果为 True，则 X 将被复制；否则，可能会被覆盖。\n",
    "       \n",
    "方法：fit, predict, score\n",
    "\n",
    "属性（可以使用 估计器.属性 进行访问）：\n",
    "coef_       模型的系数（权重），表示每个特征对目标值的影响。\n",
    "intercept_    模型的截距，表示当所有特征为零时的预测值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c7c3a730",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正规方程的权重系数为：\n",
      "[ 0.82307315  0.12362376 -0.25208115  0.29331978  0.00288605 -0.04115082\n",
      " -0.9030782  -0.87714227]\n",
      "正规方程的偏置（截距）为：\n",
      "2.070364167958359\n"
     ]
    }
   ],
   "source": [
    "# 先导包\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# 准备数据\n",
    "california= fetch_california_housing()\n",
    "\n",
    "# 划分数据集\n",
    "x_train, x_test, y_train,y_test = train_test_split(california.data, california.target, random_state=37)\n",
    "\n",
    "# 特征预处理：标准化\n",
    "transfer = StandardScaler()\n",
    "x_train = transfer.fit_transform(x_train)\n",
    "x_test = transfer.transform(x_test)\n",
    "\n",
    "# 正规方程估计器\n",
    "estimator = LinearRegression()\n",
    "estimator.fit(x_train, y_train)\n",
    "\n",
    "# 得出模型\n",
    "print(f\"正规方程的权重系数为：\\n{estimator.coef_}\")\n",
    "print(f\"正规方程的偏置（截距）为：\\n{estimator.intercept_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a30fdcd1",
   "metadata": {},
   "source": [
    "## 1.2 梯度下降：sklearn.linear_model.SGDRegressor（多次拟合）"
   ]
  },
  {
   "cell_type": "raw",
   "id": "12beb934",
   "metadata": {},
   "source": [
    "导包语法：from sklearn.linear_model import SGDRegressor\n",
    "主要参数： loss  损失函数，传入str，可以是 'squared_loss'（均方误差）、'huber'、'epsilon_insensitive' 等。\n",
    "        fit_intercept   是否计算截距，传入bool,默认值为True。\n",
    "        random_state    随机种子，用于可重复的结果。\n",
    "        learning_rate   学习率调度方式，传入str，默认值为'invscaling'，选项包括 'constant'、'optimal'、'invscaling' 和 'adaptive'。\n",
    "        eta0         初始学习率，传入float,默认值0.01（仅在学习率为 'constant' 或 'invscaling' 时有效）。\n",
    "        max_iter       最大迭代次数，传入int, default=1000\n",
    "\n",
    "主要方法：fit(X, y)        训练模型 \n",
    "       predict(X)        进行预测。\n",
    "       partial_fit(X, y)   用于增量学习，适合大规模数据集。\n",
    "       score(X, y)       计算模型的 R² 得分。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6cdc8ee0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "梯度下降的权重系数为：\n",
      "[ 0.80852936  0.12979843 -0.18931008  0.26667587  0.01630997 -0.04075928\n",
      " -0.90610236 -0.90317095]\n",
      "梯度下降的偏置（截距）为：\n",
      "[2.06456281]\n"
     ]
    }
   ],
   "source": [
    "# 先导包\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "\n",
    "# 准备数据\n",
    "california= fetch_california_housing()\n",
    "\n",
    "# 划分数据集\n",
    "x_train, x_test, y_train,y_test = train_test_split(california.data, california.target, random_state=37)\n",
    "\n",
    "# 特征预处理：标准化\n",
    "transfer = StandardScaler()\n",
    "x_train = transfer.fit_transform(x_train)\n",
    "x_test = transfer.transform(x_test)\n",
    "\n",
    "# 梯度下降估计器\n",
    "estimator = SGDRegressor()\n",
    "estimator.fit(x_train, y_train)\n",
    "\n",
    "# 得出模型\n",
    "print(f\"梯度下降的权重系数为：\\n{estimator.coef_}\")\n",
    "print(f\"梯度下降的偏置（截距）为：\\n{estimator.intercept_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3af5805",
   "metadata": {},
   "source": [
    "## 1.3 回归性能评估"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1feaea6",
   "metadata": {},
   "source": [
    "### 1.3.1 均方误差（Mean Squared Error, MSE）\n",
    "\n",
    "均方误差主要用于衡量预测值与真实值之间的差距，它是回归问题中最常见的损失函数之一。"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a7b4711d",
   "metadata": {},
   "source": [
    "导包语法：from sklearn.metrics import mean_squared_error\n",
    "\n",
    "语法：mean_squared_error(y_true, y_pred, squared=True)\n",
    "\n",
    "参数说明： y_true: 真实值（目标值），可以是数组或列表。\n",
    "        y_pred: 预测值，格式与 y_true 相同。\n",
    "        squared: 布尔值，默认为 True。如果为 True，返回均方误差；如果为 False，返回均方根误差（Root Mean Squared Error, RMSE）。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "334aadd4",
   "metadata": {},
   "source": [
    "下求 ： 上述正规方程和梯度下降的均方误差"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a716de14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正规方程的权重系数为：\n",
      "[ 0.82307315  0.12362376 -0.25208115  0.29331978  0.00288605 -0.04115082\n",
      " -0.9030782  -0.87714227]\n",
      "正规方程的偏置（截距）为：\n",
      "2.070364167958359\n",
      "\n",
      "预测房价为：\n",
      "[1.32323615 1.64298298 0.50423978 ... 2.7116062  2.44334217 1.45822224]\n",
      "均方误差为：\n",
      "0.5096750578766663\n"
     ]
    }
   ],
   "source": [
    "# 先导包\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# 准备数据\n",
    "california= fetch_california_housing()\n",
    "\n",
    "# 划分数据集\n",
    "x_train, x_test, y_train,y_test = train_test_split(california.data, california.target, random_state=37)\n",
    "\n",
    "# 特征预处理：标准化\n",
    "transfer = StandardScaler()\n",
    "x_train = transfer.fit_transform(x_train)\n",
    "x_test = transfer.transform(x_test)\n",
    "\n",
    "# 正规方程估计器\n",
    "estimator = LinearRegression()\n",
    "estimator.fit(x_train, y_train)\n",
    "\n",
    "# 得出模型\n",
    "print(f\"正规方程的权重系数为：\\n{estimator.coef_}\")\n",
    "print(f\"正规方程的偏置（截距）为：\\n{estimator.intercept_}\")\n",
    "\n",
    "# 均方误差评估\n",
    "y_predict = estimator.predict(x_test)\n",
    "print(f\"\\n预测房价为：\\n{y_predict}\")\n",
    "error = mean_squared_error(y_test, y_predict, squared=True)\n",
    "print(f\"均方误差为：\\n{error}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3225a850",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "梯度下降的权重系数为：\n",
      "[ 0.80385689  0.14155439 -0.23518863  0.29028435  0.01983835 -0.03270678\n",
      " -0.89186901 -0.8675867 ]\n",
      "梯度下降的偏置（截距）为：\n",
      "[2.06039786]\n",
      "\n",
      "预测房价为：\n",
      "[1.34128328 1.65326828 0.49952932 ... 2.67501059 2.46601871 1.47420974]\n",
      "均方误差为：\n",
      "0.5115981182579206\n"
     ]
    }
   ],
   "source": [
    "# 先导包\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# 准备数据\n",
    "california= fetch_california_housing()\n",
    "\n",
    "# 划分数据集\n",
    "x_train, x_test, y_train,y_test = train_test_split(california.data, california.target, random_state=37)\n",
    "\n",
    "# 特征预处理：标准化\n",
    "transfer = StandardScaler()\n",
    "x_train = transfer.fit_transform(x_train)\n",
    "x_test = transfer.transform(x_test)\n",
    "\n",
    "# 梯度下降估计器\n",
    "estimator = SGDRegressor()\n",
    "estimator.fit(x_train, y_train)\n",
    "\n",
    "# 得出模型\n",
    "print(f\"梯度下降的权重系数为：\\n{estimator.coef_}\")\n",
    "print(f\"梯度下降的偏置（截距）为：\\n{estimator.intercept_}\")\n",
    "\n",
    "# 均方误差评估\n",
    "y_predict = estimator.predict(x_test)\n",
    "print(f\"\\n预测房价为：\\n{y_predict}\")\n",
    "error = mean_squared_error(y_test, y_predict, squared=True)\n",
    "print(f\"均方误差为：\\n{error}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc367ec6",
   "metadata": {},
   "source": [
    "# 2 欠拟合和过拟合\n",
    "\n",
    "欠拟合：由于模型过于简单，无法捕捉数据中的重要模式，导致模型在训练集和测试集上的性能都很差。特征为模型的训练误差和测试误差都很高。\n",
    "\n",
    "过拟合：由于模型过于复杂，捕捉了数据中的噪声而非真实模式，导致模型在训练集上表现良好，但在测试集上表现不佳。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47cea3d8",
   "metadata": {},
   "source": [
    "## 2.1 正则化（应对过拟合）"
   ]
  },
  {
   "cell_type": "raw",
   "id": "003c5f18",
   "metadata": {},
   "source": [
    "正则化公式：Loss = MSE + λ（惩罚系数）×惩罚项\n",
    "L1 正则化: 形成一个菱形的约束区域，导致在坐标轴上有交点，从而使某些参数为零。\n",
    "L2 正则化: 形成一个圆形的约束区域，导致所有参数都被压缩，但不会完全为零。\n",
    "    \n",
    "由于L1正则化过于粗暴（直接将某些参数调为0），故而更多选择L2正则化（压缩某些参数）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0975629a",
   "metadata": {},
   "source": [
    "## 2.2 岭回归（带有L2正则化的线性回归）"
   ]
  },
  {
   "cell_type": "raw",
   "id": "68c4b07e",
   "metadata": {},
   "source": [
    "导包语法：from sklearn.linear_model import Ridge\n",
    "\n",
    "参数： alpha         默认为1，即惩罚系数，也可以叫做正则化力度，取值为0-1，或者1-10\n",
    "     fit_intercept    是否计算截距，传入bool,默认值为True。\n",
    "     normalize       数据是否进行标准化，默认为False  \n",
    "     max_iter       最大迭代次数，传入int, default=1000\n",
    "\n",
    "属性（可以使用 估计器.属性 进行访问）：\n",
    "coef_       模型的系数（权重），表示每个特征对目标值的影响。\n",
    "intercept_    模型的截距，表示当所有特征为零时的预测值。\n",
    "\n",
    "注意： 正则化力度越大，权重系数越小\n",
    "     正则化力度越小，权重系数越大"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4a2a9f06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "岭回归的权重系数为：\n",
      "[ 0.82304135  0.12371089 -0.25191297  0.29309756  0.00291594 -0.04115598\n",
      " -0.90224737 -0.87630386]\n",
      "岭回归的偏置（截距）为：\n",
      "2.070364167958359\n",
      "\n",
      "预测房价为：\n",
      "[1.32318056 1.64341054 0.50469793 ... 2.71140393 2.44326814 1.45817639]\n",
      "岭回归的均方误差为：\n",
      "0.5096820050449234\n"
     ]
    }
   ],
   "source": [
    "# 先导包\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# 获取数据\n",
    "california = fetch_california_housing()\n",
    "\n",
    "# 划分数据集\n",
    "x_train, x_test, y_train, y_test = train_test_split(california.data, california.target, random_state=37)\n",
    "\n",
    "# 数据标准化\n",
    "transfer = StandardScaler()\n",
    "x_train = transfer.fit_transform(x_train)\n",
    "x_test = transfer.transform(x_test)\n",
    "\n",
    "# 预估器\n",
    "estimator = Ridge()\n",
    "estimator.fit(x_train, y_train)\n",
    "\n",
    "# 得出模型\n",
    "print(f\"岭回归的权重系数为：\\n{estimator.coef_}\")\n",
    "print(f\"岭回归的偏置（截距）为：\\n{estimator.intercept_}\")\n",
    "\n",
    "# 均方误差评估\n",
    "y_predict = estimator.predict(x_test)\n",
    "print(f\"\\n预测房价为：\\n{y_predict}\")\n",
    "error = mean_squared_error(y_test, y_predict, squared=True)\n",
    "print(f\"岭回归的均方误差为：\\n{error}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2aa31ce",
   "metadata": {},
   "source": [
    "# 3 逻辑回归"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ebf3a77",
   "metadata": {},
   "source": [
    "与线性回归之间有联系，但是逻辑回归属于分类算法！\n",
    "\n",
    "常用于解决二分类问题，即：是/否"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3ade8a32",
   "metadata": {},
   "source": [
    "导包语法： from sklearn.linear_model import LogisticRegression\n",
    "参数说明： random_state      随机数种子，用于结果可重复性。\n",
    "        penalty         正则化类型，默认为 'l2'。可选值为 'l1'、'l2'、'elasticnet' 或 None。\n",
    "        solver          优化算法。可选值包括 'newton-cg'、'lbfgs'、'liblinear'、'sag'、'saga'。\n",
    "                     'liblinear' 适用于小型数据集，'lbfgs' 和 'sag' 更适合大型数据集。\n",
    "\n",
    "常用方法： fit(X, y)        训练模型，其中X为特征值，y为目标值\n",
    "        predict(X)       预测模型，其中X为测试集的特征值\n",
    "        score(X, y)       计算模型在给定数据集上的准确率，其中X为测试集预测值，y为测试集目标值\n",
    "\n",
    "常用属性（使用 .属性 进行访问）   coef_             模型系数，表示每个特征的权重\n",
    "                        intercept_          截距项\n",
    "                        set_params(**params)   设置模型参数。\n",
    "                        get_params(deep=True)   获取模型的参数。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2ad34dd",
   "metadata": {},
   "source": [
    "## 3.1 逻辑回归对癌症分类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "94f21e5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "估计器的模型系数为：\n",
      "[[ 0.51033318 -0.05682976  0.41828484  0.35540447  0.06736245  0.2895899\n",
      "   0.50260394  0.24346757  0.2497843 ]]\n",
      "估计器的截距为：[-9.75960563]\n",
      "y_predict:\n",
      " [4 4 4 2 2 4 4 4 2 2 2 4 2 2 4 4 4 2 2 2 2 2 2 2 4 2 4 2 4 2 4 2 4 4 4 2 4\n",
      " 2 2 2 4 2 4 4 2 2 2 4 4 4 2 2 2 4 4 2 2 2 4 2 2 2 2 2 2 2 4 2 4 4 2 4 4 2\n",
      " 2 4 4 2 2 2 4 4 2 4 2 2 4 4 2 4 4 2 2 4 2 2 4 2 2 2 4 2 2 2 4 2 4 2 2 2 2\n",
      " 2 2 2 2 2 2 2 4 4 2 2 2 2 2 2 4 2 4 2 2 4 2 4 4 2 4]\n",
      "直接比对真实值和预测值:\n",
      " 132    True\n",
      "98     True\n",
      "152    True\n",
      "507    True\n",
      "504    True\n",
      "       ... \n",
      "490    True\n",
      "55     True\n",
      "436    True\n",
      "133    True\n",
      "636    True\n",
      "Name: Class, Length: 137, dtype: bool\n",
      "准确率为：\n",
      " 0.9635036496350365\n"
     ]
    }
   ],
   "source": [
    "# 先导包\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# 1、读取数据\n",
    "path = \"https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/breast-cancer-wisconsin.data\"\n",
    "column_name = ['Sample code number', 'Clump Thickness', 'Uniformity of Cell Size', 'Uniformity of Cell Shape',\n",
    "                   'Marginal Adhesion', 'Single Epithelial Cell Size', 'Bare Nuclei', 'Bland Chromatin',\n",
    "                   'Normal Nucleoli', 'Mitoses', 'Class']\n",
    "data = pd.read_csv(path, names=column_name)\n",
    "\n",
    "# 2、缺失值处理\n",
    "# 1）替换  -->  np.nan\n",
    "data = data.replace(to_replace=\"?\", value=np.nan)\n",
    "# 2）删除缺失样本\n",
    "data.dropna(inplace=True)\n",
    "\n",
    "# 3、筛选特征值和目标值\n",
    "x = data.iloc[:, 1:-1]\n",
    "y = data[\"Class\"]\n",
    "\n",
    "# 4、划分数据集\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, random_state=37, test_size=0.2)\n",
    "\n",
    "# 5、预估器流程\n",
    "estimator = LogisticRegression()  # 实例化\n",
    "estimator.fit(x_train, y_train)\n",
    "\n",
    "# 6、逻辑回归的模型参数：回归系数和偏置\n",
    "coef = estimator.coef_   # 模型系数\n",
    "intercept = estimator.intercept_  # 截距\n",
    "print(f\"估计器的模型系数为：\\n{coef}\")\n",
    "print(f\"估计器的截距为：{intercept}\")\n",
    "\n",
    "# 7、模型评估\n",
    "# 方法1：直接比对真实值和预测值\n",
    "y_predict = estimator.predict(x_test)\n",
    "print(\"y_predict:\\n\", y_predict)\n",
    "print(\"直接比对真实值和预测值:\\n\", y_test == y_predict)\n",
    "\n",
    "# 方法2：计算准确率\n",
    "score = estimator.score(x_test, y_test)\n",
    "print(\"准确率为：\\n\", score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17fd4910",
   "metadata": {},
   "source": [
    "# 4 分类的评估方法"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b84c243b",
   "metadata": {},
   "source": [
    "## 4.1 样本均衡的情况"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0088723d",
   "metadata": {},
   "source": [
    "### 4.1.1 分类评估报告：sklearn.metrics.classification_report方法"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2812494a",
   "metadata": {},
   "source": [
    "精确率（Precision）：预测结果为正例样本中真实为正例的比例。\n",
    "\n",
    "召回率（Recall）：真实为正例的样本中预测结果为正例的比例。（即正例被标记为正例的比例）\n",
    "\n",
    "F1评分： 公式： 2 (Precision × Recall) / (Precision + Recall)， 反映模型的稳健性\n",
    "\n",
    "上述三个模型主要用于二分类问题，但是多分类问题也可以使用"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1ed619f0",
   "metadata": {},
   "source": [
    "注：无需实例化\n",
    "导包语法：from sklearn.metrics import classification_report\n",
    "\n",
    "参数说明\n",
    "y_true:  测试集的真实标签。\n",
    "y_pred:  分类器的预测标签。\n",
    "labels:  以列表的形式传入，指定类别对应的数字，未提供视为全部都指定。\n",
    "target_names:  以列表的形式传入，目标类别的名称\n",
    "\n",
    "注意：返回值为精确率、召回率、F1分数、每个类别的实际样本数（Support）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "92870f81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "估计器的模型系数为：\n",
      "[[ 0.51033318 -0.05682976  0.41828484  0.35540447  0.06736245  0.2895899\n",
      "   0.50260394  0.24346757  0.2497843 ]]\n",
      "估计器的截距为：[-9.75960563]\n",
      "y_predict:\n",
      " [4 4 4 2 2 4 4 4 2 2 2 4 2 2 4 4 4 2 2 2 2 2 2 2 4 2 4 2 4 2 4 2 4 4 4 2 4\n",
      " 2 2 2 4 2 4 4 2 2 2 4 4 4 2 2 2 4 4 2 2 2 4 2 2 2 2 2 2 2 4 2 4 4 2 4 4 2\n",
      " 2 4 4 2 2 2 4 4 2 4 2 2 4 4 2 4 4 2 2 4 2 2 4 2 2 2 4 2 2 2 4 2 4 2 2 2 2\n",
      " 2 2 2 2 2 2 2 4 4 2 2 2 2 2 2 4 2 4 2 2 4 2 4 4 2 4]\n",
      "直接比对真实值和预测值:\n",
      " 132    True\n",
      "98     True\n",
      "152    True\n",
      "507    True\n",
      "504    True\n",
      "       ... \n",
      "490    True\n",
      "55     True\n",
      "436    True\n",
      "133    True\n",
      "636    True\n",
      "Name: Class, Length: 137, dtype: bool\n",
      "准确率为：\n",
      " 0.9635036496350365\n",
      "\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "          良性       0.96      0.98      0.97        82\n",
      "          恶性       0.96      0.95      0.95        55\n",
      "\n",
      "    accuracy                           0.96       137\n",
      "   macro avg       0.96      0.96      0.96       137\n",
      "weighted avg       0.96      0.96      0.96       137\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 先导包\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# 1、读取数据\n",
    "path = \"https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/breast-cancer-wisconsin.data\"\n",
    "column_name = ['Sample code number', 'Clump Thickness', 'Uniformity of Cell Size', 'Uniformity of Cell Shape',\n",
    "                   'Marginal Adhesion', 'Single Epithelial Cell Size', 'Bare Nuclei', 'Bland Chromatin',\n",
    "                   'Normal Nucleoli', 'Mitoses', 'Class']\n",
    "data = pd.read_csv(path, names=column_name)\n",
    "\n",
    "# 2、缺失值处理\n",
    "# 1）替换  -->  np.nan\n",
    "data = data.replace(to_replace=\"?\", value=np.nan)\n",
    "# 2）删除缺失样本\n",
    "data.dropna(inplace=True)\n",
    "\n",
    "# 3、筛选特征值和目标值\n",
    "x = data.iloc[:, 1:-1]\n",
    "y = data[\"Class\"]\n",
    "\n",
    "# 4、划分数据集\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, random_state=37, test_size=0.2)\n",
    "\n",
    "# 5、预估器流程\n",
    "estimator = LogisticRegression()  # 实例化\n",
    "estimator.fit(x_train, y_train)\n",
    "\n",
    "# 6、逻辑回归的模型参数：回归系数和偏置\n",
    "coef = estimator.coef_   # 模型系数\n",
    "intercept = estimator.intercept_  # 截距\n",
    "print(f\"估计器的模型系数为：\\n{coef}\")\n",
    "print(f\"估计器的截距为：{intercept}\")\n",
    "\n",
    "# 7、模型评估\n",
    "# 方法1：直接比对真实值和预测值\n",
    "y_predict = estimator.predict(x_test)\n",
    "print(\"y_predict:\\n\", y_predict)\n",
    "print(\"直接比对真实值和预测值:\\n\", y_test == y_predict)\n",
    "\n",
    "# 方法2：计算准确率\n",
    "score = estimator.score(x_test, y_test)\n",
    "print(\"准确率为：\\n\", score)\n",
    "\n",
    "# 8、使用分类评估方法\n",
    "# 先导包\n",
    "from sklearn.metrics import classification_report\n",
    "report = classification_report(y_test, y_predict, labels=[2, 4], target_names=[\"良性\", \"恶性\"])\n",
    "print(\"\\n\", report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d5fa69e",
   "metadata": {},
   "source": [
    "### 4.1.2 只获得精确率、召回率、F1分数"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7cf44fc2",
   "metadata": {},
   "source": [
    "导包语法：\n",
    "准确率： from sklearn.metrics import accuracy_score\n",
    "召回率： from sklearn.metrics import recall_score\n",
    "F1分数： from sklearn.metrics import f1_score\n",
    "\n",
    "参数说明： y_true: 真实标签数组\n",
    "        y_pred: 预测标签数组\n",
    "        \n",
    "        对于召回率和F1分数，还要使用以下参数：\n",
    "        average 参数选项\n",
    "            'binary':默认值，适用于二元分类，计算正类（pos_label）的指标。\n",
    "            'micro':计算全局指标，通过计算所有类别的真实正例和假正例来获得整体性能。适合大规模数据集，尤其是类别不平衡的情况。\n",
    "            'macro':计算每个类别的指标，然后取这些指标的简单算术平均值。所有类别权重相同，适合类别不平衡的情况。\n",
    "            'weighted':计算每个类别的指标，并根据每个类别的样本数量加权平均。适合不平衡数据集，能够更好地反映整体性能。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "aab54a27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "估计器的模型系数为：\n",
      "[[ 0.51033318 -0.05682976  0.41828484  0.35540447  0.06736245  0.2895899\n",
      "   0.50260394  0.24346757  0.2497843 ]]\n",
      "估计器的截距为：[-9.75960563]\n",
      "y_predict:\n",
      " [4 4 4 2 2 4 4 4 2 2 2 4 2 2 4 4 4 2 2 2 2 2 2 2 4 2 4 2 4 2 4 2 4 4 4 2 4\n",
      " 2 2 2 4 2 4 4 2 2 2 4 4 4 2 2 2 4 4 2 2 2 4 2 2 2 2 2 2 2 4 2 4 4 2 4 4 2\n",
      " 2 4 4 2 2 2 4 4 2 4 2 2 4 4 2 4 4 2 2 4 2 2 4 2 2 2 4 2 2 2 4 2 4 2 2 2 2\n",
      " 2 2 2 2 2 2 2 4 4 2 2 2 2 2 2 4 2 4 2 2 4 2 4 4 2 4]\n",
      "直接比对真实值和预测值:\n",
      " 132    True\n",
      "98     True\n",
      "152    True\n",
      "507    True\n",
      "504    True\n",
      "       ... \n",
      "490    True\n",
      "55     True\n",
      "436    True\n",
      "133    True\n",
      "636    True\n",
      "Name: Class, Length: 137, dtype: bool\n",
      "准确率为：\n",
      " 0.9635036496350365\n",
      "\n",
      "逻辑回归的准确率为: 0.9635036496350365\n",
      "\n",
      "逻辑回归的召回率为: 0.9605321507760531\n",
      "\n",
      "逻辑回归的F1分数为: 0.9619127050319711\n"
     ]
    }
   ],
   "source": [
    "# 先导包\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# 1、读取数据\n",
    "path = \"https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/breast-cancer-wisconsin.data\"\n",
    "column_name = ['Sample code number', 'Clump Thickness', 'Uniformity of Cell Size', 'Uniformity of Cell Shape',\n",
    "                   'Marginal Adhesion', 'Single Epithelial Cell Size', 'Bare Nuclei', 'Bland Chromatin',\n",
    "                   'Normal Nucleoli', 'Mitoses', 'Class']\n",
    "data = pd.read_csv(path, names=column_name)\n",
    "\n",
    "# 2、缺失值处理\n",
    "# 1）替换  -->  np.nan\n",
    "data = data.replace(to_replace=\"?\", value=np.nan)\n",
    "# 2）删除缺失样本\n",
    "data.dropna(inplace=True)\n",
    "\n",
    "# 3、筛选特征值和目标值\n",
    "x = data.iloc[:, 1:-1]\n",
    "y = data[\"Class\"]\n",
    "\n",
    "# 4、划分数据集\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, random_state=37, test_size=0.2)\n",
    "\n",
    "# 5、预估器流程\n",
    "estimator = LogisticRegression()  # 实例化\n",
    "estimator.fit(x_train, y_train)\n",
    "\n",
    "# 6、逻辑回归的模型参数：回归系数和偏置\n",
    "coef = estimator.coef_   # 模型系数\n",
    "intercept = estimator.intercept_  # 截距\n",
    "print(f\"估计器的模型系数为：\\n{coef}\")\n",
    "print(f\"估计器的截距为：{intercept}\")\n",
    "\n",
    "# 7、模型评估\n",
    "# 方法1：直接比对真实值和预测值\n",
    "y_predict = estimator.predict(x_test)\n",
    "print(\"y_predict:\\n\", y_predict)\n",
    "print(\"直接比对真实值和预测值:\\n\", y_test == y_predict)\n",
    "\n",
    "# 方法2：计算准确率\n",
    "score = estimator.score(x_test, y_test)\n",
    "print(\"准确率为：\\n\", score)\n",
    "\n",
    "# 8、直接使用单一方法获取\n",
    "from sklearn.metrics import accuracy_score\n",
    "precision = accuracy_score(y_test, y_predict)\n",
    "print(f'\\n逻辑回归的准确率为: {precision}\\n')\n",
    "\n",
    "from sklearn.metrics import recall_score\n",
    "recall = recall_score(y_test, y_predict, average='macro')\n",
    "print(f'逻辑回归的召回率为: {recall}\\n')\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "f1 = f1_score(y_test, y_predict, average='macro')\n",
    "print(f'逻辑回归的F1分数为: {f1}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f67eec83",
   "metadata": {},
   "source": [
    "## 4.2 样本不均衡的情况"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9c18fe6",
   "metadata": {},
   "source": [
    "### 4.2.1 ROC曲线、AUC值"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a81ffd0",
   "metadata": {},
   "source": [
    "TPR = TP / (TP + FN) ：所有真实类别为1的样本中，预测类别为1的比例。\n",
    "\n",
    "FPR = FP / (FP + TN) ：所有真实类别为0的样本中，预测类别为1的比例。\n",
    "\n",
    "ROC曲线的横轴就是FPRate，纵轴就是TPRate，当两者相等时，意味着AUC值为0.5。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a70427ad",
   "metadata": {},
   "source": [
    "AUC值 即 ROC曲线的面积\n",
    "\n",
    "AUC的最高值为1，最低值为0.5，越接近于1越好。\n",
    "\n",
    "AUC只能用于二分类问题，且非常适用于评价样本不均衡中的分类器性能"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1047f91a",
   "metadata": {},
   "source": [
    "导包语法：from sklearn.metrics import roc_auc_score\n",
    "\n",
    "参数说明：roc_auc_score(y_true, y_score)\n",
    "       y_true：每个样本的真实类别，必须为0，1标记。（这里0是反例，1是正例）\n",
    "       y_score：预测得分，可以是正类的估计概率、置信值或者分类器方法的返回值。\n",
    "       \n",
    "注意： y_true可以使用 np.where 转换为0、1标记的形式。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9d4cd83d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "所得AUC值为：0.960532150776053\n"
     ]
    }
   ],
   "source": [
    "# 先导包\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# 1、读取数据\n",
    "path = \"https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/breast-cancer-wisconsin.data\"\n",
    "column_name = ['Sample code number', 'Clump Thickness', 'Uniformity of Cell Size', 'Uniformity of Cell Shape',\n",
    "                   'Marginal Adhesion', 'Single Epithelial Cell Size', 'Bare Nuclei', 'Bland Chromatin',\n",
    "                   'Normal Nucleoli', 'Mitoses', 'Class']\n",
    "data = pd.read_csv(path, names=column_name)\n",
    "\n",
    "# 2、缺失值处理\n",
    "# 1）替换  -->  np.nan\n",
    "data = data.replace(to_replace=\"?\", value=np.nan)\n",
    "# 2）删除缺失样本\n",
    "data.dropna(inplace=True)\n",
    "\n",
    "# 3、筛选特征值和目标值\n",
    "x = data.iloc[:, 1:-1]\n",
    "y = data[\"Class\"]\n",
    "\n",
    "# 4、划分数据集\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, random_state=37, test_size=0.2)\n",
    "\n",
    "# 5、预估器流程\n",
    "estimator = LogisticRegression()  # 实例化\n",
    "estimator.fit(x_train, y_train)\n",
    "\n",
    "# 6、展示AUC值的使用方法\n",
    "from sklearn.metrics import roc_auc_score\n",
    "# 使用np.where将y_test转换为要求的格式\n",
    "y_true = np.where(y_test > 3, 1, 0)\n",
    "# 准备预测数据\n",
    "y_predict = estimator.predict(x_test)\n",
    "# 使用roc_auc_score方法\n",
    "auc_value = roc_auc_score(y_true, y_predict)\n",
    "print(f\"所得AUC值为：{auc_value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e539bc7",
   "metadata": {},
   "source": [
    "# 5 模型的保存和加载（joblib库）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "545603c6",
   "metadata": {},
   "source": [
    "## 5.1 模型的保存和加载API"
   ]
  },
  {
   "cell_type": "raw",
   "id": "33de932b",
   "metadata": {},
   "source": [
    "导包语法：import joblib\n",
    "保存语法：joblib.dump(rf, \"test.pkl\")\n",
    "       参数说明：rf 表示要存档的变量名\n",
    "              test.pkl是文件名\n",
    "加载语法：estimator = joblib.load(\"test.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be2508d3",
   "metadata": {},
   "source": [
    "### 5.1.1 模型保存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8fe9b01f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['岭回归.pkl']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 先导包\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "# 获取数据\n",
    "california = fetch_california_housing()\n",
    "\n",
    "# 划分数据集\n",
    "x_train, x_test, y_train, y_test = train_test_split(california.data, california.target, random_state=37)\n",
    "\n",
    "# 数据标准化\n",
    "transfer = StandardScaler()\n",
    "x_train = transfer.fit_transform(x_train)\n",
    "x_test = transfer.transform(x_test)\n",
    "\n",
    "# 预估器\n",
    "estimator = Ridge()\n",
    "estimator.fit(x_train, y_train)\n",
    "\n",
    "# 保存模型\n",
    "import joblib\n",
    "joblib.dump(estimator, \"岭回归.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e074fa1",
   "metadata": {},
   "source": [
    "### 5.1.2 模型加载（这里不需要导入岭回归类）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "22f10018",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "预测房价为：\n",
      "[1.32318056 1.64341054 0.50469793 ... 2.71140393 2.44326814 1.45817639]\n",
      "岭回归的均方误差为：\n",
      "0.5096820050449234\n"
     ]
    }
   ],
   "source": [
    "# 先导包\n",
    "import joblib\n",
    "\n",
    "# 加载模型\n",
    "estimator = joblib.load(\"岭回归.pkl\")\n",
    "\n",
    "# 加载数据，注意对于这个数据集，之前的岭回归均方误差为0.5096820050449234\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# 获取数据\n",
    "california = fetch_california_housing()\n",
    "\n",
    "# 划分数据集\n",
    "x_train, x_test, y_train, y_test = train_test_split(california.data, california.target, random_state=37)\n",
    "\n",
    "# 数据标准化\n",
    "transfer = StandardScaler()\n",
    "x_train = transfer.fit_transform(x_train)\n",
    "x_test = transfer.transform(x_test)\n",
    "\n",
    "# 均方误差评估\n",
    "y_predict = estimator.predict(x_test)\n",
    "print(f\"\\n预测房价为：\\n{y_predict}\")\n",
    "error = mean_squared_error(y_test, y_predict, squared=True)\n",
    "print(f\"岭回归的均方误差为：\\n{error}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "533360c6",
   "metadata": {},
   "source": [
    "# 6 无监督算法 K-means算法"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d208df23",
   "metadata": {},
   "source": [
    "导包语法：from sklearn.cluster import KMeans\n",
    "参数说明：n_clusters      即k值，开始时的聚类中心数量\n",
    "       init          初始化方法，默认为 \"k-means++\"\n",
    "       labels_        默认标记的类型，可以和真实值比较（不是值比较）"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
